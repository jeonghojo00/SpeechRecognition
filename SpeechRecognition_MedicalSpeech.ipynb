{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpeechRecognition_MedicalSpeech",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1v9FcT4_pk1V2TVjJJ2bo32MhKbgHqSD5",
      "authorship_tag": "ABX9TyM2lJs4+HVTpZOoe06fzavF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonghojo00/SpeechRecognition/blob/main/SpeechRecognition_MedicalSpeech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWeYRH8U9tWi"
      },
      "source": [
        "# Target 1: Convert Wav file into Text\n",
        " Target 2: Analyze symptoms with text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hba77M7HLU_V"
      },
      "source": [
        "## Step 0. Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjyxXokk906a",
        "outputId": "9688efef-e56b-4e07-c363-d37cb23f15f5"
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ki8pALfQDcH"
      },
      "source": [
        "# Change current directory\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/SpeechRecognition/MedicalSpeech')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GVfCfoJOkRN",
        "outputId": "bc63d78d-722d-42a4-d90a-95be82b28684"
      },
      "source": [
        "# Import Overview csv file\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#Analyze Data\n",
        "def explore_data(df):\n",
        "    print(f\"The data contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "    print('\\n')\n",
        "    print('Dataset columns:',df.columns)\n",
        "    print('\\n')\n",
        "    print(df.info())\n",
        "\n",
        "filename = \"overview-of-recordings.csv\"\n",
        "overview = pd.read_csv(filename)\n",
        "explore_data(overview)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The data contains 6661 rows and 13 columns.\n",
            "\n",
            "\n",
            "Dataset columns: Index(['audio_clipping', 'audio_clipping:confidence',\n",
            "       'background_noise_audible', 'background_noise_audible:confidence',\n",
            "       'overall_quality_of_the_audio', 'quiet_speaker',\n",
            "       'quiet_speaker:confidence', 'speaker_id', 'file_download', 'file_name',\n",
            "       'phrase', 'prompt', 'writer_id'],\n",
            "      dtype='object')\n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6661 entries, 0 to 6660\n",
            "Data columns (total 13 columns):\n",
            " #   Column                               Non-Null Count  Dtype  \n",
            "---  ------                               --------------  -----  \n",
            " 0   audio_clipping                       6661 non-null   object \n",
            " 1   audio_clipping:confidence            6661 non-null   float64\n",
            " 2   background_noise_audible             6661 non-null   object \n",
            " 3   background_noise_audible:confidence  6661 non-null   float64\n",
            " 4   overall_quality_of_the_audio         6661 non-null   float64\n",
            " 5   quiet_speaker                        6661 non-null   object \n",
            " 6   quiet_speaker:confidence             6661 non-null   float64\n",
            " 7   speaker_id                           6661 non-null   int64  \n",
            " 8   file_download                        6661 non-null   object \n",
            " 9   file_name                            6661 non-null   object \n",
            " 10  phrase                               6661 non-null   object \n",
            " 11  prompt                               6661 non-null   object \n",
            " 12  writer_id                            6661 non-null   int64  \n",
            "dtypes: float64(4), int64(2), object(7)\n",
            "memory usage: 676.6+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvQTzFTFM-au"
      },
      "source": [
        "## Step 1. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmT1Dx8xV7xm"
      },
      "source": [
        "##### Preprocessing includes following actions:\n",
        "1. Load audio file\n",
        "2. Load csv file\n",
        "3. Convert audio file into spectrogram feature, mfcc feature\n",
        "4/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE1GoZ3Gc3aO"
      },
      "source": [
        "### Step 1-1. Import train, validate, and test corpus json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhAwKR_cQZO7"
      },
      "source": [
        "import json\n",
        "\n",
        "f = open('train_corpus.json',)\n",
        "train_corpus = json.load(f)\n",
        "\n",
        "f = open('valid_corpus.json',)\n",
        "validate_corpus = json.load(f)\n",
        "\n",
        "f = open('test_corpus.json',)\n",
        "test_corpus = json.load(f)\n",
        "  "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOP8yKkAXhqm"
      },
      "source": [
        "### Step 1-2. Define functions for various wavelengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yav0lvdjfjjz"
      },
      "source": [
        "# Transform wav into signal and rate\n",
        "import librosa, librosa.display\n",
        "\n",
        "signal, sr = librosa.load(train_corpus[0][\"key\"], sr=22050)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "3VOKuotmhNIg",
        "outputId": "55df037f-8dd1-4ead-f33f-caf30bb2d85a"
      },
      "source": [
        "# WaveForm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "librosa.display.waveplot(signal, sr=sr)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7VT5nLmng0"
      },
      "source": [
        "import librosa, librosa.display\n",
        "import numpy as np\n",
        "\n",
        "def to_spectrogram(file_path, n_fft=320, sr=22050, plot=False):\n",
        "    # Transform wav into signal and rate\n",
        "    signal, sr = librosa.load(file_path, sr=sr)\n",
        "\n",
        "    # Spectrogram\n",
        "\n",
        "    #n_fft=2048: Number of samples per fft\n",
        "    #hop_length = 512 # Amount we shift to the right\n",
        "\n",
        "    hop_length = 128\n",
        "\n",
        "  \n",
        "\n",
        "    stft = librosa.core.stft(signal, hop_length=hop_length, n_fft = n_fft)\n",
        "    spectrogram = np.abs(stft)\n",
        "\n",
        "    log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
        "    \n",
        "    if plot:\n",
        "        # Plot the result\n",
        "        librosa.display.specshow(log_spectrogram, sr=sr, hop_length=hop_length)\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "    \n",
        "    tr_log_spectrogram = np.transpose(log_spectrogram)\n",
        "    return tr_log_spectrogram"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjOdHcAvmsm9"
      },
      "source": [
        "import numpy as np\n",
        "def normalize(feature, eps=1e-14):\n",
        "        \"\"\" Center a feature using the mean and std\n",
        "        Params:\n",
        "            feature (numpy.ndarray): Feature to normalize\n",
        "        \"\"\"\n",
        "        feats_mean = np.mean(log_spectrogram, axis=0)\n",
        "        feats_std = np.std(log_spectrogram, axis=0)\n",
        "        return (feature - feats_mean) / (feats_std + eps)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sMD3S1VWYrz"
      },
      "source": [
        "def to_mfcc(file_path, sr=22050, plot=False):\n",
        "    # MFCCs\n",
        "    signal, sr = librosa.load(train_corpus[0][\"key\"], sr=sr)\n",
        "    MFCCs = librosa.feature.mfcc(signal, n_fft=n_fft, hop_length=hop_length, n_mfcc=13)\n",
        "\n",
        "    if plot:\n",
        "        librosa.display.specshow(MFCCs, sr=sr, hop_length=hop_length)\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Pp2-Lejd7j"
      },
      "source": [
        "def to_spectrum(file_path, sr=22050, plot=False):\n",
        "    # Spectrum\n",
        "    # Transform wav into signal and rate\n",
        "    signal, sr = librosa.load(train_corpus[0][\"key\"], sr=sr)\n",
        "\n",
        "    fft = np.fft.fft(signal)\n",
        "    magnitude = np.abs(fft)\n",
        "    frequency = np.linspace(0, sr, len(magnitude))\n",
        "\n",
        "    # Because once we pass over the half, we repeat the same information. we use only the first half\n",
        "    left_frequency = frequency[:int(len(frequency)/2)]\n",
        "    left_magnitude = magnitude[:int(len(magnitude)/2)]\n",
        "\n",
        "    if plot:\n",
        "\n",
        "        plt.plot(left_frequency, left_magnitude)\n",
        "        plt.xlabel(\"Frequency\")\n",
        "        plt.ylabel(\"Magnitude\")\n",
        "        plt.show()\n",
        "    "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ymHLA2Phq6"
      },
      "source": [
        "### Step 1.3. Prepare Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAJTwd5jk65g"
      },
      "source": [
        "SPACE_TOKEN = '<space>'\n",
        "SPACE_INDEX = 0\n",
        "FIRST_INDEX = ord('a') - 1  # 0 is reserved to space\n",
        "\n",
        "def convert_inputs_to_ctc_format(audio_path, feature, text, samplerate=22050):\n",
        "    # Convert auido file into audio feature in 3D\n",
        "    ## Load audio file into selected feature\n",
        "    if feature == \"spectrogram\":\n",
        "        inputs = to_spectrogram(audio_path, sr=samplerate)\n",
        "    elif feature == \"mfcc\":\n",
        "        inputs = to_mfcc(audio_path, sr=samplerate)\n",
        "    elif feature == \"spectrum\":\n",
        "        inputs = to_spectrum(audio_path, sr=samplerate)\n",
        "    \n",
        "    ## Change inputs into 3D\n",
        "    train_inputs = np.asarray(inputs[np.newaxis, :])\n",
        "    train_inputs = (train_inputs - np.mean(train_inputs)) / np.std(train_inputs)\n",
        "    train_seq_len = [train_inputs.shape[1]]\n",
        "\n",
        "    # Get only the words between [a-z] and replace period for none\n",
        "    original = ' '.join(text.strip().lower().split(' '))\\\n",
        "                  .replace('.', '').replace('?', '').replace(',','').replace(\"'\", '').replace('!', '').replace('-', '')\n",
        "\n",
        "    targets = original.replace(' ', '  ')\n",
        "    targets = targets.split(' ')\n",
        "\n",
        "    # Adding blank label\n",
        "    targets = np.hstack([SPACE_TOKEN if x == '' else list(x) for x in targets])\n",
        "\n",
        "    # Transform char into index\n",
        "    targets = np.asarray([SPACE_INDEX if x == SPACE_TOKEN else ord(x) - FIRST_INDEX\n",
        "                          for x in targets])\n",
        "\n",
        "    return train_inputs, train_seq_len, targets, original\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FguPmjK74mOr"
      },
      "source": [
        "def pad_sequences(sequences, maxlen=None, dtype=np.float32,\n",
        "                  padding='post', truncating='post', value=0.):\n",
        "    '''Pads each sequence to the same length: the length of the longest\n",
        "    sequence.\n",
        "        If maxlen is provided, any sequence longer than maxlen is truncated to\n",
        "        maxlen. Truncation happens off either the beginning or the end\n",
        "        (default) of the sequence. Supports post-padding (default) and\n",
        "        pre-padding.\n",
        "        Args:\n",
        "            sequences: list of lists where each element is a sequence\n",
        "            maxlen: int, maximum length\n",
        "            dtype: type to cast the resulting sequence.\n",
        "            padding: 'pre' or 'post', pad either before or after each sequence.\n",
        "            truncating: 'pre' or 'post', remove values from sequences larger\n",
        "            than maxlen either in the beginning or in the end of the sequence\n",
        "            value: float, value to pad the sequences to the desired value.\n",
        "        Returns\n",
        "            x: numpy array with dimensions (number_of_sequences, maxlen)\n",
        "            lengths: numpy array with the original sequence lengths\n",
        "    '''\n",
        "    lengths = np.asarray([len(s) for s in sequences], dtype=np.int64)\n",
        "\n",
        "    nb_samples = len(sequences)\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "\n",
        "    # take the sample shape from the first non empty sequence\n",
        "    # checking for consistency in the main loop below.\n",
        "    sample_shape = tuple()\n",
        "    for s in sequences:\n",
        "        if len(s) > 0:\n",
        "            sample_shape = np.asarray(s).shape[1:]\n",
        "            break\n",
        "\n",
        "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if len(s) == 0:\n",
        "            continue  # empty list was found\n",
        "        if truncating == 'pre':\n",
        "            trunc = s[-maxlen:]\n",
        "        elif truncating == 'post':\n",
        "            trunc = s[:maxlen]\n",
        "        else:\n",
        "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
        "\n",
        "        # check `trunc` has expected shape\n",
        "        trunc = np.asarray(trunc, dtype=dtype)\n",
        "        if trunc.shape[1:] != sample_shape:\n",
        "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
        "                             (trunc.shape[1:], idx, sample_shape))\n",
        "\n",
        "        if padding == 'post':\n",
        "            x[idx, :len(trunc)] = trunc\n",
        "        elif padding == 'pre':\n",
        "            x[idx, -len(trunc):] = trunc\n",
        "        else:\n",
        "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
        "    return x, lengths"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E0tSwJVwqcV"
      },
      "source": [
        "SPACE_TOKEN = '<space>'\n",
        "SPACE_INDEX = 0\n",
        "FIRST_INDEX = ord('a') - 1  # 0 is reserved to space\n",
        "\n",
        "def decode_text(y_predict, original):\n",
        "    # Decode list of integer into the text\n",
        "    aligned_original_string = ''\n",
        "    aligned_decoded_string = ''\n",
        "\n",
        "    str_decoded = ''.join([chr(x) for x in np.asarray(y_predict) + FIRST_INDEX])\n",
        "\n",
        "    str_decoded = str_decoded.replace(chr(ord('z') + 1), '')\n",
        "    # Replacing space label to space\n",
        "    str_decoded = str_decoded.replace(chr(ord('a') - 1), ' ')\n",
        "    maxlen = max(len(original), len(str_decoded))\n",
        "    aligned_original_string += str(original).ljust(maxlen)\n",
        "    aligned_decoded_string += str(str_decoded).ljust(maxlen)\n",
        "\n",
        "    print('- Original: %s' % (aligned_original_string))\n",
        "    print('- Decoded : %s' % (aligned_decoded_string))\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0af_8hhXqLaa"
      },
      "source": [
        "def sparse_tuple_from(sequences, dtype=np.int32):\n",
        "    \"\"\"Create a sparse representention of x.\n",
        "    Args:\n",
        "        sequences: a list of lists of type dtype where each element is a sequence\n",
        "    Returns:\n",
        "        A tuple with (indices, values, shape)\n",
        "    \"\"\"\n",
        "    indices = []\n",
        "    values = []\n",
        "\n",
        "    for n, seq in enumerate(sequences):\n",
        "        indices.extend(zip([n] * len(seq), range(len(seq))))\n",
        "        values.extend(seq)\n",
        "\n",
        "    indices = np.asarray(indices, dtype=np.int64)\n",
        "    values = np.asarray(values, dtype=dtype)\n",
        "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n",
        "\n",
        "    return indices, values, shape"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg-W4R-TZ98S"
      },
      "source": [
        "## Step 2. Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OnOHS8ylaBqz",
        "outputId": "65185d8b-11eb-4f92-e0f4-b8ebaddc9f5d"
      },
      "source": [
        "'''\n",
        "Recurrent Neural Network works weell in sequential data better than Feed Forward Neural Network\n",
        "Because speech is sequential data, we use Recurrent Neural Network\n",
        "'''\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nRecurrent Neural Network works weell in sequential data better than Feed Forward Neural Network\\nBecause speech is sequential data, we use Recurrent Neural Network\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWHX-M1siGQd"
      },
      "source": [
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "def add_ctc_loss(input_to_softmax):\n",
        "    the_labels = Input(name='the_labels', shape=(None,), dtype='float32')\n",
        "    input_lengths = Input(name='input_length', shape=(1,), dtype='int64')\n",
        "    label_lengths = Input(name='label_length', shape=(1,), dtype='int64')\n",
        "    output_lengths = Lambda(input_to_softmax.output_length)(input_lengths)\n",
        "    # CTC loss is implemented in a lambda layer\n",
        "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
        "        [input_to_softmax.output, the_labels, output_lengths, label_lengths])\n",
        "    model = Model(\n",
        "        inputs=[input_to_softmax.input, the_labels, input_lengths, label_lengths], \n",
        "        outputs=loss_out)\n",
        "    return model\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U-kI-dzSZI8"
      },
      "source": [
        "#### AutoGenerator function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuE5LnXgav8O"
      },
      "source": [
        "RNG_SEED = 123\n",
        "\n",
        "\n",
        "class AudioGenerator():\n",
        "    def __init__(self, step=10, window=20, max_freq=8000, mfcc_dim=13,\n",
        "        minibatch_size=20, desc_file=None, spectrogram=True, max_duration=10.0, \n",
        "        sort_by_duration=False, n_fft=320):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            step (int): Step size in milliseconds between windows (for spectrogram ONLY)\n",
        "            window (int): FFT window size in milliseconds (for spectrogram ONLY)\n",
        "            max_freq (int): Only FFT bins corresponding to frequencies between\n",
        "                [0, max_freq] are returned (for spectrogram ONLY)\n",
        "            desc_file (str, optional): Path to a JSON-line file that contains\n",
        "                labels and paths to the audio files. If this is None, then\n",
        "                load metadata right away\n",
        "        \"\"\"\n",
        "        self.n_fft = n_fft\n",
        "        self.feat_dim = int(n_fft/2) +1\n",
        "        '''\n",
        "        self.feat_dim = calc_feat_dim(window, max_freq)\n",
        "        '''\n",
        "        self.mfcc_dim = mfcc_dim\n",
        "        self.feats_mean = np.zeros((self.feat_dim,))\n",
        "        self.feats_std = np.ones((self.feat_dim,))\n",
        "        self.rng = random.Random(RNG_SEED)\n",
        "        if desc_file is not None:\n",
        "            self.load_metadata_from_desc_file(desc_file)\n",
        "        self.step = step\n",
        "        self.window = window\n",
        "        self.max_freq = max_freq\n",
        "        self.cur_train_index = 0\n",
        "        self.cur_valid_index = 0\n",
        "        self.cur_test_index = 0\n",
        "        self.max_duration=max_duration\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.spectrogram = spectrogram\n",
        "        self.sort_by_duration = sort_by_duration\n",
        "\n",
        "    def get_batch(self, partition):\n",
        "        \"\"\" Obtain a batch of train, validation, or test data\n",
        "        \"\"\"\n",
        "        if partition == 'train':\n",
        "            audio_paths = self.train_audio_paths\n",
        "            cur_index = self.cur_train_index\n",
        "            texts = self.train_texts\n",
        "        elif partition == 'valid':\n",
        "            audio_paths = self.valid_audio_paths\n",
        "            cur_index = self.cur_valid_index\n",
        "            texts = self.valid_texts\n",
        "        elif partition == 'test':\n",
        "            audio_paths = self.test_audio_paths\n",
        "            cur_index = self.test_valid_index\n",
        "            texts = self.test_texts\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition. \"\n",
        "                \"Must be train/validation\")\n",
        "\n",
        "        features = [self.normalize(self.featurize(a)) for a in \n",
        "            audio_paths[cur_index:cur_index+self.minibatch_size]]\n",
        "        print(\"\")\n",
        "        print(\"Current Index: \", self.cur_train_index)\n",
        "        print(\"shape of first feature:\", features[0].shape)\n",
        "        print(features[0])\n",
        "\n",
        "        # calculate necessary sizes\n",
        "        max_length = max([features[i].shape[0] \n",
        "            for i in range(0, self.minibatch_size)])\n",
        "        max_string_length = max([len(texts[cur_index+i]) \n",
        "            for i in range(0, self.minibatch_size)])\n",
        "        \n",
        "        # initialize the arrays\n",
        "        X_data = np.zeros([self.minibatch_size, max_length, self.feat_dim*self.spectrogram + self.mfcc_dim*(not self.spectrogram)])\n",
        "        labels = np.ones([self.minibatch_size, max_string_length]) * 28\n",
        "        input_length = np.zeros([self.minibatch_size, 1])\n",
        "        label_length = np.zeros([self.minibatch_size, 1])\n",
        "        \n",
        "        for i in range(0, self.minibatch_size):\n",
        "            # calculate X_data & input_length\n",
        "            feat = features[i]\n",
        "            input_length[i] = feat.shape[0]\n",
        "            X_data[i, :feat.shape[0], :] = feat\n",
        "\n",
        "            # calculate labels & label_length\n",
        "            label = np.array(text_to_int_sequence(texts[cur_index+i])) \n",
        "            labels[i, :len(label)] = label\n",
        "            label_length[i] = len(label)\n",
        " \n",
        "        # return the arrays\n",
        "        outputs = {'ctc': np.zeros([self.minibatch_size])}\n",
        "        inputs = {'the_input': X_data, \n",
        "                  'the_labels': labels, \n",
        "                  'input_length': input_length, \n",
        "                  'label_length': label_length \n",
        "                 }\n",
        "        print(\"get_batch starts\")\n",
        "        print(\"minibatch size: \", self.minibatch_size)\n",
        "        print(\"number of intpus: \", len(inputs[\"the_input\"]))\n",
        "        print(\"shape of first input: \", inputs[\"the_input\"][0].shape)\n",
        "        print(\"shape of second input: \", inputs[\"the_input\"][1].shape)\n",
        "        print(\"shape of first labels: \", inputs[\"the_labels\"][0].shape)\n",
        "        print(\"shape of second labels: \", inputs[\"the_labels\"][1].shape)\n",
        "\n",
        "        print(\"get_batch ends\")\n",
        "        return (inputs, outputs)\n",
        "\n",
        "    def shuffle_data_by_partition(self, partition):\n",
        "        \"\"\" Shuffle the training or validation data\n",
        "        \"\"\"\n",
        "        if partition == 'train':\n",
        "            self.train_audio_paths, self.train_durations, self.train_texts = shuffle_data(\n",
        "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
        "        elif partition == 'valid':\n",
        "            self.valid_audio_paths, self.valid_durations, self.valid_texts = shuffle_data(\n",
        "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition. \"\n",
        "                \"Must be train/validation\")\n",
        "\n",
        "    def sort_data_by_duration(self, partition):\n",
        "        \"\"\" Sort the training or validation sets by (increasing) duration\n",
        "        \"\"\"\n",
        "        if partition == 'train':\n",
        "            self.train_audio_paths, self.train_durations, self.train_texts = sort_data(\n",
        "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
        "        elif partition == 'valid':\n",
        "            self.valid_audio_paths, self.valid_durations, self.valid_texts = sort_data(\n",
        "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition. \"\n",
        "                \"Must be train/validation\")\n",
        "\n",
        "    def data_for_fit(self, partition):\n",
        "        \"\"\"Obtain a dataset for partition\n",
        "        \"\"\"\n",
        "        if partition == 'train':\n",
        "            audio_paths = self.train_audio_paths\n",
        "            texts = self.train_texts\n",
        "        elif partition == 'valid':\n",
        "            audio_paths = self.valid_audio_paths\n",
        "            texts = self.valid_texts\n",
        "        elif partition == 'test':\n",
        "            audio_paths = self.test_audio_paths\n",
        "            texts = self.test_texts\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition. \"\n",
        "                \"Must be train/validation\")\n",
        "\n",
        "        features = [self.normalize(self.featurize(a)) for a in audio_paths]\n",
        "        labels = np.array([text_to_int_sequence(a) for a in texts])\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "            \n",
        "    def next_train(self):\n",
        "        \"\"\" Obtain a batch of training data\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            ret = self.get_batch('train')\n",
        "            self.cur_train_index += self.minibatch_size\n",
        "            if self.cur_train_index >= len(self.train_texts) - self.minibatch_size:\n",
        "                self.cur_train_index = 0\n",
        "                self.shuffle_data_by_partition('train')\n",
        "            yield ret    \n",
        "\n",
        "    def next_valid(self):\n",
        "        \"\"\" Obtain a batch of validation data\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            ret = self.get_batch('valid')\n",
        "            self.cur_valid_index += self.minibatch_size\n",
        "            if self.cur_valid_index >= len(self.valid_texts) - self.minibatch_size:\n",
        "                self.cur_valid_index = 0\n",
        "                self.shuffle_data_by_partition('valid')\n",
        "            yield ret\n",
        "\n",
        "    def next_test(self):\n",
        "        \"\"\" Obtain a batch of test data\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            ret = self.get_batch('test')\n",
        "            self.cur_test_index += self.minibatch_size\n",
        "            if self.cur_test_index >= len(self.test_texts) - self.minibatch_size:\n",
        "                self.cur_test_index = 0\n",
        "            yield ret\n",
        "\n",
        "    def load_train_data(self, desc_file='train_corpus.json'):\n",
        "        self.load_metadata_from_desc_file(desc_file, 'train')\n",
        "        self.fit_train()\n",
        "        if self.sort_by_duration:\n",
        "            self.sort_data_by_duration('train')\n",
        "\n",
        "    def load_validation_data(self, desc_file='valid_corpus.json'):\n",
        "        self.load_metadata_from_desc_file(desc_file, 'validation')\n",
        "        if self.sort_by_duration:\n",
        "            self.sort_data_by_duration('valid')\n",
        "\n",
        "    def load_test_data(self, desc_file='test_corpus.json'):\n",
        "        self.load_metadata_from_desc_file(desc_file, 'test')\n",
        "    \n",
        "    def load_metadata_from_desc_file(self, desc_file, partition):\n",
        "        \"\"\" Read metadata from a JSON-line file\n",
        "            (possibly takes long, depending on the filesize)\n",
        "        Params:\n",
        "            desc_file (str):  Path to a JSON-line file that contains labels and\n",
        "                paths to the audio files\n",
        "            partition (str): One of 'train', 'validation' or 'test'\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        f = open(desc_file)\n",
        "        file_loaded = json.load(f)\n",
        "        \n",
        "        audio_paths, durations, texts = [], [], []\n",
        "        for speech in file_loaded:\n",
        "            if float(speech[\"duration\"]) > self.max_duration:\n",
        "                continue\n",
        "            audio_paths.append(speech[\"key\"])\n",
        "            durations.append(speech[\"duration\"])\n",
        "\n",
        "            audioText = speech[\"text\"].lower()\n",
        "            audioText = re.sub('[^a-zA-Z0-9 \\n]', '', audioText)\n",
        "            audioText = audioText.strip()\n",
        "            '''\n",
        "            audioText = speech[\"text\"].lower().replace('.', '').replace('?', '').replace(',','').replace(\"'\", '').replace('!', '').replace('-', '')\n",
        "            '''\n",
        "            audioText = convert_num_to_words(audioText)\n",
        "            texts.append(audioText)\n",
        "\n",
        "        if partition == 'train':\n",
        "            self.train_audio_paths = audio_paths\n",
        "            self.train_durations = durations\n",
        "            self.train_texts = texts\n",
        "        elif partition == 'validation':\n",
        "            self.valid_audio_paths = audio_paths\n",
        "            self.valid_durations = durations\n",
        "            self.valid_texts = texts\n",
        "        elif partition == 'test':\n",
        "            self.test_audio_paths = audio_paths\n",
        "            self.test_durations = durations\n",
        "            self.test_texts = texts\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition to load metadata. \"\n",
        "             \"Must be train/validation/test\")\n",
        "            \n",
        "    def fit_train(self, k_samples=100):\n",
        "        \"\"\" Estimate the mean and std of the features from the training set\n",
        "        Params:\n",
        "            k_samples (int): Use this number of samples for estimation\n",
        "        \"\"\"\n",
        "        k_samples = min(k_samples, len(self.train_audio_paths))\n",
        "        samples = self.rng.sample(self.train_audio_paths, k_samples)\n",
        "        feats = [self.featurize(s) for s in samples]\n",
        "        feats = np.vstack(feats)\n",
        "        self.feats_mean = np.mean(feats, axis=0)\n",
        "        self.feats_std = np.std(feats, axis=0)\n",
        "        \n",
        "    def featurize(self, audio_clip):\n",
        "        \"\"\" For a given audio clip, calculate the corresponding feature\n",
        "        Params:\n",
        "            audio_clip (str): Path to the audio clip\n",
        "        \"\"\"\n",
        "        if self.spectrogram:\n",
        "            return to_spectrogram(audio_clip, n_fft=self.n_fft)\n",
        "            '''\n",
        "            return spectrogram_from_file(\n",
        "                audio_clip, step=self.step, window=self.window,\n",
        "                max_freq=self.max_freq)\n",
        "            '''\n",
        "\n",
        "        else:\n",
        "            (rate, sig) = wav.read(audio_clip)\n",
        "            return mfcc(sig, rate, numcep=self.mfcc_dim)\n",
        "\n",
        "    def normalize(self, feature, eps=1e-14):\n",
        "        \"\"\" Center a feature using the mean and std\n",
        "        Params:\n",
        "            feature (numpy.ndarray): Feature to normalize\n",
        "        \"\"\"\n",
        "        return (feature - self.feats_mean) / (self.feats_std + eps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiSFmetNyMvV",
        "outputId": "67fe9848-5b5a-4d10-f6f7-300bb25e6cdc"
      },
      "source": [
        "pip install python_speech_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python_speech_features in /usr/local/lib/python3.7/dist-packages (0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAvJPt8Q7KzT",
        "outputId": "3b4ccda1-6011-4a61-9ac7-eb195d801978"
      },
      "source": [
        "pip install num2words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.7/dist-packages (0.5.10)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICWxyiXqtbi0"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "from python_speech_features import mfcc\n",
        "import librosa\n",
        "import scipy.io.wavfile as wav\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO2Kv-Ket3qs"
      },
      "source": [
        "\"\"\"\n",
        "Defines two dictionaries for converting \n",
        "between text and integer sequences.\n",
        "\"\"\"\n",
        "\n",
        "char_map_str = \"\"\"\n",
        "' 0\n",
        "<SPACE> 1\n",
        "a 2\n",
        "b 3\n",
        "c 4\n",
        "d 5\n",
        "e 6\n",
        "f 7\n",
        "g 8\n",
        "h 9\n",
        "i 10\n",
        "j 11\n",
        "k 12\n",
        "l 13\n",
        "m 14\n",
        "n 15\n",
        "o 16\n",
        "p 17\n",
        "q 18\n",
        "r 19\n",
        "s 20\n",
        "t 21\n",
        "u 22\n",
        "v 23\n",
        "w 24\n",
        "x 25\n",
        "y 26\n",
        "z 27\n",
        "\"\"\"\n",
        "# the \"blank\" character is mapped to 28\n",
        "\n",
        "char_map = {}\n",
        "index_map = {}\n",
        "for line in char_map_str.strip().split('\\n'):\n",
        "    ch, index = line.split()\n",
        "    char_map[ch] = int(index)\n",
        "    index_map[int(index)+1] = ch\n",
        "index_map[2] = ' '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9WTMZnetium"
      },
      "source": [
        "import numpy as np\n",
        "import soundfile\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import num2words #!pip install num2words\n",
        "\n",
        "def shuffle_data(audio_paths, durations, texts):\n",
        "    \"\"\" Shuffle the data (called after making a complete pass through \n",
        "        training or validation data during the training process)\n",
        "    Params:\n",
        "        audio_paths (list): Paths to audio clips\n",
        "        durations (list): Durations of utterances for each audio clip\n",
        "        texts (list): Sentences uttered in each audio clip\n",
        "    \"\"\"\n",
        "    p = np.random.permutation(len(audio_paths))\n",
        "    audio_paths = [audio_paths[i] for i in p] \n",
        "    durations = [durations[i] for i in p] \n",
        "    texts = [texts[i] for i in p]\n",
        "    return audio_paths, durations, texts\n",
        "\n",
        "def sort_data(audio_paths, durations, texts):\n",
        "    \"\"\" Sort the data by duration \n",
        "    Params:\n",
        "        audio_paths (list): Paths to audio clips\n",
        "        durations (list): Durations of utterances for each audio clip\n",
        "        texts (list): Sentences uttered in each audio clip\n",
        "    \"\"\"\n",
        "    p = np.argsort(durations).tolist()\n",
        "    audio_paths = [audio_paths[i] for i in p]\n",
        "    durations = [durations[i] for i in p] \n",
        "    texts = [texts[i] for i in p]\n",
        "    return audio_paths, durations, texts\n",
        "\n",
        "def vis_train_features(index=0):\n",
        "    \"\"\" Visualizing the data point in the training set at the supplied index\n",
        "    \"\"\"\n",
        "    # obtain spectrogram\n",
        "    audio_gen = AudioGenerator(spectrogram=True)\n",
        "    audio_gen.load_train_data()\n",
        "    vis_audio_path = audio_gen.train_audio_paths[index]\n",
        "    vis_spectrogram_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
        "    # obtain mfcc\n",
        "    audio_gen = AudioGenerator(spectrogram=False)\n",
        "    audio_gen.load_train_data()\n",
        "    vis_mfcc_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
        "    # obtain text label\n",
        "    vis_text = audio_gen.train_texts[index]\n",
        "    # obtain raw audio\n",
        "    vis_raw_audio, _ = librosa.load(vis_audio_path)\n",
        "    # print total number of training examples\n",
        "    print('There are %d total training examples.' % len(audio_gen.train_audio_paths))\n",
        "    # return labels for plotting\n",
        "    return vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path\n",
        "\n",
        "\n",
        "def plot_raw_audio(vis_raw_audio):\n",
        "    # plot the raw audio signal\n",
        "    fig = plt.figure(figsize=(12,3))\n",
        "    ax = fig.add_subplot(111)\n",
        "    steps = len(vis_raw_audio)\n",
        "    ax.plot(np.linspace(1, steps, steps), vis_raw_audio)\n",
        "    plt.title('Audio Signal')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.show()\n",
        "\n",
        "def plot_mfcc_feature(vis_mfcc_feature):\n",
        "    # plot the MFCC feature\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    im = ax.imshow(vis_mfcc_feature, cmap=plt.cm.jet, aspect='auto')\n",
        "    plt.title('Normalized MFCC')\n",
        "    plt.ylabel('Time')\n",
        "    plt.xlabel('MFCC Coefficient')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    ax.set_xticks(np.arange(0, 13, 2), minor=False);\n",
        "    plt.show()\n",
        "\n",
        "def plot_spectrogram_feature(vis_spectrogram_feature):\n",
        "    # plot the normalized spectrogram\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    im = ax.imshow(vis_spectrogram_feature, cmap=plt.cm.jet, aspect='auto')\n",
        "    plt.title('Normalized Spectrogram')\n",
        "    plt.ylabel('Time')\n",
        "    plt.xlabel('Frequency')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    plt.show()\n",
        "\n",
        "def convert_num_to_words(utterance):\n",
        "      utterance = ' '.join([num2words.num2words(i) if i.isdigit() else i for i in utterance.split()])\n",
        "      return utterance\n",
        "\n",
        "def calc_feat_dim(window, max_freq):\n",
        "    return int(0.001 * window * max_freq) + 1\n",
        "\n",
        "def conv_output_length(input_length, filter_size, border_mode, stride,\n",
        "                       dilation=1):\n",
        "    \"\"\" Compute the length of the output sequence after 1D convolution along\n",
        "        time. Note that this function is in line with the function used in\n",
        "        Convolution1D class from Keras.\n",
        "    Params:\n",
        "        input_length (int): Length of the input sequence.\n",
        "        filter_size (int): Width of the convolution kernel.\n",
        "        border_mode (str): Only support `same` or `valid`.\n",
        "        stride (int): Stride size used in 1D convolution.\n",
        "        dilation (int)\n",
        "    \"\"\"\n",
        "    if input_length is None:\n",
        "        return None\n",
        "    assert border_mode in {'same', 'valid'}\n",
        "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
        "    if border_mode == 'same':\n",
        "        output_length = input_length\n",
        "    elif border_mode == 'valid':\n",
        "        output_length = input_length - dilated_filter_size + 1\n",
        "    return (output_length + stride - 1) // stride\n",
        "\n",
        "\n",
        "def get_spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n",
        "    \"\"\"\n",
        "    Compute the spectrogram for a real signal.\n",
        "    The parameters follow the naming convention of\n",
        "    matplotlib.mlab.specgram\n",
        "\n",
        "    Args:\n",
        "        samples (1D array): input audio signal\n",
        "        fft_length (int): number of elements in fft window\n",
        "        sample_rate (scalar): sample rate\n",
        "        hop_length (int): hop length (relative offset between neighboring\n",
        "            fft windows).\n",
        "\n",
        "    Returns:\n",
        "        x (2D array): spectrogram [frequency x time]\n",
        "        freq (1D array): frequency of each row in x\n",
        "\n",
        "    Note:\n",
        "        This is a truncating computation e.g. if fft_length=10,\n",
        "        hop_length=5 and the signal has 23 elements, then the\n",
        "        last 3 elements will be truncated.\n",
        "    \"\"\"\n",
        "    assert not np.iscomplexobj(samples), \"Must not pass in complex numbers\"\n",
        "\n",
        "    window = np.hanning(fft_length)[:, None]\n",
        "    window_norm = np.sum(window**2)\n",
        "\n",
        "    # The scaling below follows the convention of\n",
        "    # matplotlib.mlab.specgram which is the same as\n",
        "    # matlabs specgram.\n",
        "    scale = window_norm * sample_rate\n",
        "\n",
        "    trunc = (len(samples) - fft_length) % hop_length\n",
        "    x = samples[:len(samples) - trunc]\n",
        "\n",
        "    # \"stride trick\" reshape to include overlap\n",
        "    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n",
        "    nstrides = (x.strides[0], x.strides[0] * hop_length)\n",
        "    x = as_strided(x, shape=nshape, strides=nstrides)\n",
        "\n",
        "    # window stride sanity check\n",
        "    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n",
        "\n",
        "    # broadcast window, compute fft over columns and square mod\n",
        "    x = np.fft.rfft(x * window, axis=0)\n",
        "    x = np.absolute(x)**2\n",
        "\n",
        "    # scale, 2.0 for everything except dc and fft_length/2\n",
        "    x[1:-1, :] *= (2.0 / scale)\n",
        "    x[(0, -1), :] /= scale\n",
        "\n",
        "    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n",
        "\n",
        "    return x, freqs\n",
        "\n",
        "\n",
        "def spectrogram_from_file(filename, step=10, window=20, max_freq=None,\n",
        "                          eps=1e-14):\n",
        "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
        "    Params:\n",
        "        filename (str): Path to the audio file\n",
        "        step (int): Step size in milliseconds between windows\n",
        "        window (int): FFT window size in milliseconds\n",
        "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
        "            [0, max_freq] are returned\n",
        "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
        "    \"\"\"\n",
        "    with soundfile.SoundFile(filename) as sound_file:\n",
        "        audio = sound_file.read(dtype='float32')\n",
        "        sample_rate = sound_file.samplerate\n",
        "        print(audio.ndim)\n",
        "        if audio.ndim >= 2:\n",
        "            audio = np.mean(audio, 1)\n",
        "        if max_freq is None:\n",
        "            max_freq = sample_rate / 2\n",
        "\n",
        "        if max_freq > sample_rate / 2:\n",
        "            print(\"Error at: \", filename)\n",
        "            raise ValueError(\"max_freq must not be greater than half of \"\n",
        "                             \" sample rate\")\n",
        "        if step > window:\n",
        "            print(\"Error at: \", filename)\n",
        "            raise ValueError(\"step size must not be greater than window size\")\n",
        "\n",
        "        hop_length = int(0.001 * step * sample_rate)\n",
        "        fft_length = int(0.001 * window * sample_rate)\n",
        "        pxx, freqs = get_spectrogram(audio, fft_length=fft_length, sample_rate=sample_rate, hop_length=hop_length)\n",
        "        ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
        "    \n",
        "    spectroGram = np.transpose(np.log(pxx[:ind, :] + eps))\n",
        "\n",
        "    print(\"Filename         : \", filename)\n",
        "    print(\"PXX shape: \", pxx.shape, \", IND: \", ind, \", freq: \", freqs.shape, \", Max_freq: \", max_freq)\n",
        "    print(\"Sample Rate: \", sample_rate, \", Hop Length: \", hop_length, \", FFT Length: \", fft_length)\n",
        "    print(\"Spectrogram shape: \", spectroGram.shape)\n",
        "\n",
        "    return spectroGram\n",
        "\n",
        "def text_to_int_sequence(text):\n",
        "    \"\"\" Convert text to an integer sequence \"\"\"\n",
        "    int_sequence = []\n",
        "    for c in text:\n",
        "        if c == ' ':\n",
        "            ch = char_map['<SPACE>']\n",
        "        else:\n",
        "            ch = char_map[c]\n",
        "        int_sequence.append(ch)\n",
        "    return int_sequence\n",
        "\n",
        "def int_sequence_to_text(int_sequence):\n",
        "    \"\"\" Convert an integer sequence to text \"\"\"\n",
        "    text = []\n",
        "    print(int_sequence)\n",
        "    for c in int_sequence:\n",
        "        ch = index_map[c]\n",
        "        text.append(ch)\n",
        "    return text\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0XK7hSQ2pLG"
      },
      "source": [
        "#### Function: Model Build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32vjQi3AC8q8"
      },
      "source": [
        "def cnn_output_length(input_length, filter_size, border_mode, stride,\n",
        "                       dilation=1):\n",
        "    \"\"\" Compute the length of the output sequence after 1D convolution along\n",
        "        time. Note that this function is in line with the function used in\n",
        "        Convolution1D class from Keras.\n",
        "    Params:\n",
        "        input_length (int): Length of the input sequence.\n",
        "        filter_size (int): Width of the convolution kernel.\n",
        "        border_mode (str): Only support `same` or `valid`.\n",
        "        stride (int): Stride size used in 1D convolution.\n",
        "        dilation (int)\n",
        "    \"\"\"\n",
        "    if input_length is None:\n",
        "        return None\n",
        "    assert border_mode in {'same', 'valid'}\n",
        "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
        "    if border_mode == 'same':\n",
        "        output_length = input_length\n",
        "    elif border_mode == 'valid':\n",
        "        output_length = input_length - dilated_filter_size + 1\n",
        "    return (output_length + stride - 1) // stride"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXkxLiPAxHSw"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
        "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM, Dropout)\n",
        "\n",
        "def final_model(input_dim, filters, kernel_size, conv_stride,\n",
        "    conv_border_mode, units, output_dim=29):\n",
        "    \"\"\" Build a deep network for speech \n",
        "    \"\"\"\n",
        "    # Main acoustic input\n",
        "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
        "    # TODO: Specify the layers in your network\n",
        "    # Add convolutional layer\n",
        "    conv_1d = Conv1D(filters, kernel_size, \n",
        "                     strides=conv_stride, \n",
        "                     padding=conv_border_mode,\n",
        "                     activation='relu',\n",
        "                     name='conv1d')(input_data)\n",
        "    # Add batch normalization\n",
        "    bn_cnn = BatchNormalization(name='bn_conv_1d')(conv_1d)\n",
        "    # Add a recurrent layer\n",
        "    dropout1 = Dropout(0.3)(bn_cnn)\n",
        "    simp_rnn1 = GRU(units, return_sequences=True)(dropout1)\n",
        "    bn_rnn1 = BatchNormalization(name='bn_conv_1d1')(simp_rnn1)\n",
        "    \n",
        "    dropout2 = Dropout(0.3)(bn_rnn1)\n",
        "    simp_rnn2 = GRU(units, return_sequences=True)(dropout2)\n",
        "    bn_rnn2 = BatchNormalization(name='bn_conv_1d2')(simp_rnn2)\n",
        "\n",
        "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
        "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn2)\n",
        "\n",
        "    # TODO: Add softmax activation layer\n",
        "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
        "    # Specify the model\n",
        "    model = Model(inputs=input_data, outputs=y_pred)\n",
        "    # TODO: Specify model.output_length\n",
        "    model.output_length = lambda x: cnn_output_length(\n",
        "        x, kernel_size, conv_border_mode, conv_stride)\n",
        "    print(model.summary())\n",
        "    print(\"Output length of model: \", model.output_length)\n",
        "    return model"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwHKQHl-3PcB",
        "outputId": "b4b5a95b-9e28-4323-a6e4-f28582b53bc4"
      },
      "source": [
        "model = final_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
        "                        filters=200,\n",
        "                        kernel_size=11, \n",
        "                        conv_stride=2,\n",
        "                        conv_border_mode='valid',\n",
        "                        units=200)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "the_input (InputLayer)       [(None, None, 161)]       0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 200)         354400    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d (BatchNormalizati (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_14 (GRU)                 (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d1 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_15 (GRU)                 (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d2 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, None, 29)          5829      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, None, 29)          0         \n",
            "=================================================================\n",
            "Total params: 845,029\n",
            "Trainable params: 843,829\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n",
            "None\n",
            "Output length of model:  <function final_model.<locals>.<lambda> at 0x7fdd2b19b320>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5gIZcX-6Ee5"
      },
      "source": [
        "import _pickle as pickle\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import (Input, Lambda)\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint   \n",
        "import os\n",
        "\n",
        "input_to_softmax = model\n",
        "train_json='train_corpus.json'\n",
        "valid_json='valid_corpus.json'\n",
        "minibatch_size=20\n",
        "spectrogram=True\n",
        "mfcc_dim=13\n",
        "optimizer=SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
        "epochs=20\n",
        "verbose=1\n",
        "sort_by_duration=False\n",
        "max_duration=10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okLRHHWl6c4l"
      },
      "source": [
        "audio_gen = AudioGenerator(minibatch_size=minibatch_size, \n",
        "        spectrogram=spectrogram, mfcc_dim=mfcc_dim, max_duration=max_duration,\n",
        "        sort_by_duration=sort_by_duration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1_HtcHr6onB"
      },
      "source": [
        "audio_gen.load_train_data(train_json) # Load Metat data from json and save in lists, then fit_train()\n",
        "audio_gen.load_validation_data(valid_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18K5yqXe6u8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de48779-1cb0-4d93-d3bf-b68fdb3bd8bf"
      },
      "source": [
        "num_train_examples=len(audio_gen.train_audio_paths)\n",
        "print(\"Printing number of Training Data\")\n",
        "print(\"Number of train data: \", num_train_examples)\n",
        "steps_per_epoch = num_train_examples//minibatch_size\n",
        "print(\"Steps per epoch: \", steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing number of Training Data\n",
            "Number of train data:  169\n",
            "Steps per epoch:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91US5qtFAr66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd65eaf8-e33f-41d3-e1d6-a67ad0f90275"
      },
      "source": [
        " # calculate validation_steps\n",
        "num_valid_samples = len(audio_gen.valid_audio_paths)\n",
        "print(\"Printing number of Validation Data\")\n",
        "print(\"Number of train data: \", num_valid_samples) \n",
        "validation_steps = num_valid_samples//minibatch_size\n",
        "print(\"Steps per epoch: \", validation_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing number of Validation Data\n",
            "Number of train data:  149\n",
            "Steps per epoch:  7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArMCU_v4BJRO"
      },
      "source": [
        "# add CTC loss to the NN specified in input_to_softmax\n",
        "model = add_ctc_loss(input_to_softmax) #Added Ctc loss\n",
        "\n",
        "# CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KXvOm3H8zcn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzRJ88E0DEq8"
      },
      "source": [
        "if not os.path.exists('results'):\n",
        "        os.makedirs('results')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "johkdpXMIVq0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "e9b7b98e-03ca-4ad5-e3a7-8012dc34b2ad"
      },
      "source": [
        "# Run by Fit\n",
        "\n",
        "epochs = 10\n",
        "verbose = 1\n",
        "pickle_path = 'model_end.pickle' \n",
        "save_model_path ='model_end.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='results/'+save_model_path, verbose=0)\n",
        "train_x, train_y = audio_gen.data_for_fit(\"train\")\n",
        "valid_x, valid_y = audio_gen.data_for_fit(\"valid\")\n",
        "\n",
        "hist = model.fit(x = train_x,\n",
        "                 y = train_y,\n",
        "                 steps_per_epoch=steps_per_epoch,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=(valid_x, valid_y),\n",
        "                 validation_steps=validation_steps,\n",
        "                 callbacks=[checkpointer], verbose=verbose)\n",
        "\n",
        "with open('results/'+pickle_path, 'wb') as f:\n",
        "        pickle.dump(hist.history, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:150: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-924da5280792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                  \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                  callbacks=[checkpointer], verbose=verbose)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpickle_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                **kwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    265\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1403\u001b[0m   \"\"\"\n\u001b[1;32m   1404\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1405\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by4NrZsGEGm1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c5ea1f7-9bad-4e0f-d86e-ec24668c9937"
      },
      "source": [
        "las# Run by Fit-Generator\n",
        "\n",
        "epochs=10\n",
        "verbose=1\n",
        "pickle_path = 'model_end.pickle' \n",
        "save_model_path ='model_end.h5'\n",
        "\n",
        "# add checkpointer\n",
        "checkpointer = ModelCheckpoint(filepath='results/'+save_model_path, verbose=0)\n",
        "\n",
        "hist = model.fit_generator(generator=audio_gen.next_train(), \n",
        "                           steps_per_epoch=steps_per_epoch,\n",
        "                           epochs=epochs, \n",
        "                           validation_data=audio_gen.next_valid(), \n",
        "                           validation_steps=validation_steps,\n",
        "                           callbacks=[checkpointer], verbose=verbose)\n",
        "\n",
        "with open('results/'+pickle_path, 'wb') as f:\n",
        "        pickle.dump(hist.history, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Index:  0\n",
            "shape of first feature: (603, 161)\n",
            "[[-0.18037342 -0.3175863  -0.42148226 ... -0.20322227 -0.18552203\n",
            "  -0.18033576]\n",
            " [-1.0269743  -0.66149443 -0.7459992  ... -0.49555477 -0.47519046\n",
            "  -0.46876925]\n",
            " [-0.9437248  -1.104789   -0.5374332  ... -0.49555477 -0.47519046\n",
            "  -0.46876925]\n",
            " ...\n",
            " [-1.1679637  -1.3224343  -0.7524987  ... -0.49555477 -0.47519046\n",
            "  -0.46876925]\n",
            " [-1.0782385  -1.0681705  -1.0878954  ... -0.49555477 -0.47519046\n",
            "  -0.46876925]\n",
            " [-0.8275997  -0.8754889  -0.7653985  ...  0.22042473  0.24840595\n",
            "   0.25148246]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1309, 161)\n",
            "shape of second input:  (1309, 161)\n",
            "get_batch ends\n",
            "Epoch 1/10\n",
            "1/8 [==>...........................] - ETA: 24s - loss: 953.9156\n",
            "Current Index:  20\n",
            "shape of first feature: (765, 161)\n",
            "[[-0.21031168 -0.4559485  -0.6435662  ... -0.0154371   0.00844488\n",
            "   0.01515031]\n",
            " [-1.0071806  -1.0217551  -0.83634126 ... -0.0154371   0.00844488\n",
            "   0.01515031]\n",
            " [-1.0071806  -0.88405645 -0.54895073 ... -0.0154371   0.00844488\n",
            "   0.01515031]\n",
            " ...\n",
            " [-1.0071806  -0.90973866 -1.4094663  ... -0.0154371   0.00844488\n",
            "   0.01515031]\n",
            " [-1.0071806  -0.8832563  -0.68478996 ... -0.0154371   0.00844488\n",
            "   0.01515031]\n",
            " [-0.48223636 -0.5904073  -0.7867244  ... -0.0154371   0.00844488\n",
            "   0.01515031]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1706, 161)\n",
            "shape of second input:  (1706, 161)\n",
            "get_batch ends\n",
            "2/8 [======>.......................] - ETA: 25s - loss: 954.2933\n",
            "Current Index:  40\n",
            "shape of first feature: (1397, 161)\n",
            "[[1.1723136  0.5505276  0.37681475 ... 0.88362706 0.9140961  0.92133385]\n",
            " [1.2532462  0.7988791  0.2565609  ... 0.88362706 0.9140961  0.92133385]\n",
            " [1.2944534  1.0188711  0.6603402  ... 0.88362706 0.9140961  0.92133385]\n",
            " ...\n",
            " [1.2260811  1.0715593  0.6178614  ... 0.88362706 0.9140961  0.92133385]\n",
            " [1.0956872  1.1380167  0.86273634 ... 0.88362706 0.9140961  0.92133385]\n",
            " [0.7538817  1.015437   0.7609267  ... 0.88362706 0.9140961  0.92133385]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1397, 161)\n",
            "shape of second input:  (1397, 161)\n",
            "get_batch ends\n",
            "3/8 [==========>...................] - ETA: 19s - loss: 876.3527\n",
            "Current Index:  60\n",
            "shape of first feature: (794, 161)\n",
            "[[ 1.0237604  -0.1569354   0.61708    ...  1.7283974   1.7591383\n",
            "   1.7768803 ]\n",
            " [ 1.0906812   0.889778    0.39606842 ...  0.76161754  0.79119277\n",
            "   0.7983582 ]\n",
            " [ 0.76563376  0.7891779   0.78783107 ...  0.76161754  0.79119277\n",
            "   0.7983582 ]\n",
            " ...\n",
            " [ 1.1654826   0.50413543  0.7495216  ...  0.76161754  0.79119277\n",
            "   0.7983582 ]\n",
            " [ 1.3570906   0.915643    0.81007355 ...  0.76161754  0.79119277\n",
            "   0.7983582 ]\n",
            " [ 1.5163808   1.3644967   1.148305   ...  1.6837794   1.7436492\n",
            "   1.7377101 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1559, 161)\n",
            "shape of second input:  (1559, 161)\n",
            "get_batch ends\n",
            "4/8 [==============>...............] - ETA: 16s - loss: 817.3453\n",
            "Current Index:  80\n",
            "shape of first feature: (677, 161)\n",
            "[[ 0.73133254  0.3426748  -0.01558636 ...  0.5147697   0.54253626\n",
            "   0.5495556 ]\n",
            " [ 0.72580636  0.27146864 -0.13186748 ...  0.5147697   0.54253626\n",
            "   0.5495556 ]\n",
            " [ 0.43948725  0.35363245 -0.29954264 ...  0.5147697   0.54253626\n",
            "   0.5495556 ]\n",
            " ...\n",
            " [ 0.57871777  0.30285943  0.07088342 ...  0.5147697   0.54253626\n",
            "   0.5495556 ]\n",
            " [ 0.2924907   0.16868816  0.08519229 ...  0.5147697   0.54253626\n",
            "   0.5495556 ]\n",
            " [ 0.61228573  0.34006822 -0.21890128 ...  1.887037    1.9204725\n",
            "   1.9265597 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1691, 161)\n",
            "shape of second input:  (1691, 161)\n",
            "get_batch ends\n",
            "5/8 [=================>............] - ETA: 12s - loss: 776.3788\n",
            "Current Index:  100\n",
            "shape of first feature: (1177, 161)\n",
            "[[ 0.8007855   0.3013398  -0.3113374  ... -0.25241828 -0.41029167\n",
            "  -0.55829424]\n",
            " [ 0.8161473   0.38285527 -0.58157337 ... -1.5477737  -2.2555935\n",
            "  -2.2502186 ]\n",
            " [ 0.7287842   0.2563381  -0.66331434 ... -2.2630084  -2.2555935\n",
            "  -2.2502186 ]\n",
            " ...\n",
            " [-1.7598534  -2.0117826  -2.150331   ... -2.2630084  -2.2555935\n",
            "  -2.2502186 ]\n",
            " [-1.7598534  -2.0117826  -2.150331   ... -2.2630084  -2.2555935\n",
            "  -2.2502186 ]\n",
            " [-1.7598534  -2.0117826  -2.150331   ... -2.2630084  -2.2555935\n",
            "  -2.2502186 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1177, 161)\n",
            "shape of second input:  (1177, 161)\n",
            "get_batch ends\n",
            "6/8 [=====================>........] - ETA: 7s - loss: 747.2797 \n",
            "Current Index:  120\n",
            "shape of first feature: (529, 161)\n",
            "[[-1.0005456 -1.3725961 -1.782465  ... -1.1470082 -1.1314168 -1.1253814]\n",
            " [-1.3861243 -1.5716962 -1.5308267 ... -1.1470082 -1.1314168 -1.1253814]\n",
            " [-1.3861243 -1.6371784 -1.6632935 ... -1.1470082 -1.1314168 -1.1253814]\n",
            " ...\n",
            " [-1.3861243 -1.6371784 -1.782465  ... -1.1470082 -1.1314168 -1.1253814]\n",
            " [-1.3861243 -1.6371784 -1.782465  ... -1.1470082 -1.1314168 -1.1253814]\n",
            " [-1.3861243 -1.6371784 -1.782465  ... -1.1470082 -1.1314168 -1.1253814]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1426, 161)\n",
            "shape of second input:  (1426, 161)\n",
            "get_batch ends\n",
            "7/8 [=========================>....] - ETA: 3s - loss: 723.8187\n",
            "Current Index:  140\n",
            "shape of first feature: (1162, 161)\n",
            "[[-0.6557158  -1.0441476  -0.64883626 ...  0.61971587  0.6482514\n",
            "   0.65533286]\n",
            " [-0.21123369 -0.49223903 -0.6513567  ...  0.61971587  0.6482514\n",
            "   0.65533286]\n",
            " [-0.2512609  -0.33004403 -0.5240243  ...  0.61971587  0.6482514\n",
            "   0.65533286]\n",
            " ...\n",
            " [-0.79447883 -1.0441476  -1.2001014  ...  0.61971587  0.6482514\n",
            "   0.65533286]\n",
            " [-0.79447883 -1.0441476  -1.2001014  ...  0.61971587  0.6482514\n",
            "   0.65533286]\n",
            " [-0.79447883 -1.0441476  -1.2001014  ...  0.61971587  0.6482514\n",
            "   0.65533286]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1706, 161)\n",
            "shape of second input:  (1706, 161)\n",
            "get_batch ends\n",
            "8/8 [==============================] - ETA: 0s - loss: 702.9123\n",
            "Current Index:  0\n",
            "shape of first feature: (641, 161)\n",
            "[[ 0.02368735 -0.19257595 -0.21876267 ...  0.27130795  0.29022074\n",
            "   0.2977974 ]\n",
            " [-0.7996249  -0.79978174 -0.93550396 ... -0.44799304 -0.42728025\n",
            "  -0.4208309 ]\n",
            " [-0.8442908  -0.8638851  -1.1557573  ... -0.44799304 -0.42728025\n",
            "  -0.4208309 ]\n",
            " ...\n",
            " [-0.9438464  -0.6561893  -0.63210696 ... -0.44799304 -0.42728025\n",
            "  -0.4208309 ]\n",
            " [-1.1520362  -0.8637632  -0.92025745 ... -0.44799304 -0.42728025\n",
            "  -0.4208309 ]\n",
            " [-0.16370822 -0.35546106 -0.4680309  ... -0.44799304 -0.42728025\n",
            "  -0.41017917]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1235, 161)\n",
            "shape of second input:  (1235, 161)\n",
            "get_batch ends\n",
            "\n",
            "Current Index:  0\n",
            "shape of first feature: (412, 161)\n",
            "[[ 0.6069059   0.20454146 -1.1559119  ...  1.5653441   1.596722\n",
            "   1.6040249 ]\n",
            " [ 0.2342172  -0.20247218  0.08229768 ...  0.02372002  0.04788889\n",
            "   0.0546175 ]\n",
            " [ 0.22191644 -0.02909246 -0.2859361  ...  0.02372002  0.04788889\n",
            "   0.0546175 ]\n",
            " ...\n",
            " [ 0.6382729   0.51971203  0.17542513 ...  0.02372002  0.04788889\n",
            "   0.0546175 ]\n",
            " [ 0.6527155   0.49387792  0.06912931 ...  0.02372002  0.04788889\n",
            "   0.0546175 ]\n",
            " [-0.05989554  0.44213703  0.22987369 ...  0.02372002  0.04788889\n",
            "   0.0546175 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1706, 161)\n",
            "shape of second input:  (1706, 161)\n",
            "get_batch ends\n",
            "\n",
            "Current Index:  20\n",
            "shape of first feature: (1324, 161)\n",
            "[[ 0.33723438  0.15687048  0.3213589  ...  1.8193336   1.8566583\n",
            "   1.8644499 ]\n",
            " [ 0.3663707   0.38679275  0.03193664 ...  1.8193336   1.8566583\n",
            "   1.8644499 ]\n",
            " [ 0.70848405  0.48946434  0.17427365 ...  1.8193336   1.8566583\n",
            "   1.8644499 ]\n",
            " ...\n",
            " [-0.3927476  -0.6414757  -0.80467236 ...  1.8193336   1.8566583\n",
            "   1.8644499 ]\n",
            " [-0.3927476  -0.6414757  -0.80467236 ...  1.8193336   1.8566583\n",
            "   1.8644499 ]\n",
            " [-0.3927476  -0.6414757  -0.80467236 ...  1.8193336   1.8566583\n",
            "   1.8644499 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1324, 161)\n",
            "shape of second input:  (1324, 161)\n",
            "get_batch ends\n",
            "\n",
            "Current Index:  20\n",
            "shape of first feature: (618, 161)\n",
            "[[ 0.72267044  0.40633678  0.10656444 ...  2.263619    2.3041987\n",
            "   2.3122532 ]\n",
            " [ 0.45054027  0.07037293 -0.10860239 ...  2.263619    2.3041987\n",
            "   2.3122532 ]\n",
            " [ 0.142761    0.08252884 -0.0873715  ...  2.263619    2.3041987\n",
            "   2.3122532 ]\n",
            " ...\n",
            " [-0.24396415 -0.4923438  -0.658223   ...  2.263619    2.3041987\n",
            "   2.3122532 ]\n",
            " [-0.24396415 -0.4923438  -0.658223   ...  2.263619    2.3041987\n",
            "   2.3122532 ]\n",
            " [-0.24396415 -0.4923438  -0.658223   ...  2.263619    2.3041987\n",
            "   2.3122532 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1473, 161)\n",
            "shape of second input:  (1473, 161)\n",
            "get_batch ends\n",
            "\n",
            "Current Index:  20\n",
            "shape of first feature: (662, 161)\n",
            "[[ 0.38346964  0.18955545  0.14121757 ...  0.4990757   0.5217086\n",
            "   0.52683586]\n",
            " [-0.19862826 -0.31199327 -0.5106879  ... -1.6604782  -1.686189\n",
            "  -1.6715021 ]\n",
            " [ 0.01036739 -0.31538737 -0.24616647 ... -2.7755454  -2.7718856\n",
            "  -2.7668142 ]\n",
            " ...\n",
            " [-1.9314932  -2.1838243  -2.3192782  ... -2.7755454  -2.7718856\n",
            "  -2.7668142 ]\n",
            " [-1.9314932  -2.1838243  -2.3192782  ... -2.7755454  -2.7718856\n",
            "  -2.7668142 ]\n",
            " [-1.9314932  -2.1838243  -2.3192782  ... -2.7755454  -2.7718856\n",
            "  -2.7668142 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1585, 161)\n",
            "shape of second input:  (1585, 161)\n",
            "get_batch ends\n",
            "\n",
            "Current Index:  20\n",
            "shape of first feature: (1089, 161)\n",
            "[[ 1.1129432   0.64143074 -0.08246411 ...  0.81471264  0.85351044\n",
            "   0.85771304]\n",
            " [ 1.0361212   0.76095104  0.3519196  ... -0.93799984 -0.92087716\n",
            "  -0.91471785]\n",
            " [ 0.38035083  0.65378106  0.4850818  ... -0.93799984 -0.92087716\n",
            "  -0.91471785]\n",
            " ...\n",
            " [-1.316131   -1.5670211  -1.7135698  ... -0.93799984 -0.92087716\n",
            "  -0.91471785]\n",
            " [-1.316131   -1.5670211  -1.7135698  ... -0.93799984 -0.92087716\n",
            "  -0.91471785]\n",
            " [-1.316131   -1.5670211  -1.7135698  ... -0.93799984 -0.92087716\n",
            "  -0.91471785]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1473, 161)\n",
            "shape of second input:  (1473, 161)\n",
            "get_batch ends\n",
            "\n",
            "Current Index:  20\n",
            "shape of first feature: (593, 161)\n",
            "[[ 0.779066    0.31735745 -1.173308   ...  0.19737226  0.21859016\n",
            "   0.22809115]\n",
            " [ 0.7701438   0.30702323 -0.9383895  ... -1.1502353  -1.1346675\n",
            "  -1.128634  ]\n",
            " [ 0.7700151   0.3072546  -1.4425938  ... -1.1502353  -1.1346675\n",
            "  -1.128634  ]\n",
            " ...\n",
            " [ 0.7724149   0.31121054 -1.3972449  ... -1.1502353  -1.1346675\n",
            "  -1.128634  ]\n",
            " [ 0.7759368   0.31435373 -1.7835287  ... -1.1502353  -1.1346675\n",
            "  -1.128634  ]\n",
            " [ 0.77941316  0.3191461  -1.7835287  ... -1.1502353  -1.1346675\n",
            "  -1.128634  ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1697, 161)\n",
            "shape of second input:  (1697, 161)\n",
            "get_batch ends\n",
            "\n",
            "Current Index:  20\n",
            "shape of first feature: (481, 161)\n",
            "[[ 0.7725511   0.3052562  -1.1914319  ...  0.38000426  0.39853463\n",
            "   0.40232256]\n",
            " [ 0.7845455   0.3268391  -1.0735204  ... -1.0395098  -1.0231309\n",
            "  -1.0170317 ]\n",
            " [ 0.7878227   0.3259961  -1.4193604  ... -1.0395098  -1.0231309\n",
            "  -1.0170317 ]\n",
            " ...\n",
            " [ 0.7826344   0.3217346  -1.1076443  ... -1.0395098  -1.0231309\n",
            "  -1.0170317 ]\n",
            " [ 0.7799368   0.31991273 -0.99789584 ... -1.0395098  -1.0231309\n",
            "  -1.0170317 ]\n",
            " [ 0.7772571   0.31763557 -1.0860912  ... -1.0395098  -1.0231309\n",
            "  -1.0170317 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1537, 161)\n",
            "shape of second input:  (1537, 161)\n",
            "get_batch ends\n",
            "\n",
            "Current Index:  20\n",
            "shape of first feature: (881, 161)\n",
            "[[ 0.788521    0.33726218 -0.8552277  ...  0.47511268  0.49417543\n",
            "   0.49611267]\n",
            " [ 0.7710492   0.306596   -1.3203481  ... -1.2496676  -1.2348285\n",
            "  -1.2288537 ]\n",
            " [ 0.76263565  0.2939734  -1.0753856  ... -1.2496676  -1.2348285\n",
            "  -1.2288537 ]\n",
            " ...\n",
            " [ 0.7757566   0.320861   -0.9335045  ... -1.2496676  -1.2348285\n",
            "  -1.2288537 ]\n",
            " [ 0.76664865  0.300517   -1.0772079  ... -1.2496676  -1.2348285\n",
            "  -1.2288537 ]\n",
            " [ 0.7708956   0.31182912 -1.1966217  ... -1.2496676  -1.2348285\n",
            "  -1.2288537 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1473, 161)\n",
            "shape of second input:  (1473, 161)\n",
            "get_batch ends\n",
            "8/8 [==============================] - 57s 8s/step - loss: 686.6518 - val_loss: 260.8578\n",
            "Epoch 2/10\n",
            "1/8 [==>...........................] - ETA: 1s - loss: 308.7824\n",
            "Current Index:  20\n",
            "shape of first feature: (456, 161)\n",
            "[[ 0.5476997   0.1654852  -0.39433897 ...  0.32812908  0.35452825\n",
            "   0.36143708]\n",
            " [ 0.30997005 -0.533849   -0.1134009  ...  0.32812908  0.35452825\n",
            "   0.36143708]\n",
            " [ 0.32085958  0.28393397  0.13817292 ...  0.32812908  0.35452825\n",
            "   0.36143708]\n",
            " ...\n",
            " [ 0.87244487  0.60448813  0.27051413 ...  0.32812908  0.35452825\n",
            "   0.36143708]\n",
            " [ 0.4584993   0.4412951   0.10771739 ...  0.32812908  0.35452825\n",
            "   0.36143708]\n",
            " [ 0.43799606  0.3298574   0.24446473 ...  0.5072947   0.5354252\n",
            "   0.5402729 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1397, 161)\n",
            "shape of second input:  (1397, 161)\n",
            "get_batch ends\n",
            "2/8 [======>.......................] - ETA: 21s - loss: 294.0960\n",
            "Current Index:  40\n",
            "shape of first feature: (912, 161)\n",
            "[[1.5967114  1.0730233  0.7548561  ... 2.627431   2.6656933  2.6765306 ]\n",
            " [1.4616187  1.1577749  0.6604921  ... 1.0359087  1.0674936  1.0748214 ]\n",
            " [1.3211553  1.0389216  0.6125176  ... 1.0359087  1.0674936  1.0748214 ]\n",
            " ...\n",
            " [1.0324072  0.89223707 0.7357973  ... 1.0359087  1.0674936  1.0748214 ]\n",
            " [1.1301858  0.9343579  0.9389866  ... 1.0359087  1.0674936  1.0748214 ]\n",
            " [1.4098477  1.1132648  1.1085585  ... 3.714343   3.7572763  3.766787  ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1706, 161)\n",
            "shape of second input:  (1706, 161)\n",
            "get_batch ends\n",
            "3/8 [==========>...................] - ETA: 20s - loss: 300.4298\n",
            "Current Index:  60\n",
            "shape of first feature: (561, 161)\n",
            "[[-1.4175147 -1.5982727 -1.5128399 ... -1.2407436 -1.2258391 -1.219859 ]\n",
            " [-1.4175147 -1.6686423 -1.813363  ... -1.2407436 -1.2258391 -1.219859 ]\n",
            " [-1.4175147 -1.6686423 -1.813363  ... -1.2407436 -1.2258391 -1.219859 ]\n",
            " ...\n",
            " [-1.1968874 -1.2398449 -1.2482685 ... -1.2407436 -1.2258391 -1.219859 ]\n",
            " [-1.4175147 -1.6686423 -1.6691525 ... -1.2407436 -1.2258391 -1.219859 ]\n",
            " [-1.4130025 -1.5475597 -1.5556278 ... -1.2407436 -1.2258391 -1.219859 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1103, 161)\n",
            "shape of second input:  (1103, 161)\n",
            "get_batch ends\n",
            "4/8 [==============>...............] - ETA: 15s - loss: 298.8313\n",
            "Current Index:  80\n",
            "shape of first feature: (794, 161)\n",
            "[[1.4544628  1.0900657  0.5797786  ... 2.9322011  2.9717646  2.9781058 ]\n",
            " [0.6734277  0.9267921  0.68006986 ... 0.84525716 0.8754451  0.8826601 ]\n",
            " [0.7172167  0.5748357  0.49240506 ... 0.84525716 0.8754451  0.8826601 ]\n",
            " ...\n",
            " [1.4133191  1.0023623  0.6689428  ... 0.84525716 0.8754451  0.8826601 ]\n",
            " [1.3757181  1.04603    0.571937   ... 0.84525716 0.8754451  0.8826601 ]\n",
            " [1.2870166  1.1652658  0.81209284 ... 1.5860138  1.5995024  1.6166488 ]]\n",
            "get_batch starts\n",
            "minibatch size:  20\n",
            "number of intpus:  20\n",
            "shape of first input:  (1691, 161)\n",
            "shape of second input:  (1691, 161)\n",
            "get_batch ends\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-271ffff6d1c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                            \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                            callbacks=[checkpointer], verbose=verbose)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpickle_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LivGg3q-E4o0"
      },
      "source": [
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from IPython.display import Audio\n",
        "\n",
        "def get_predictions(index, partition, input_to_softmax, model_path):\n",
        "    \"\"\" Print a model's decoded predictions\n",
        "    Params:\n",
        "        index (int): The example you would like to visualize\n",
        "        partition (str): One of 'train' or 'validation'\n",
        "        input_to_softmax (Model): The acoustic model\n",
        "        model_path (str): Path to saved acoustic model's weights\n",
        "    \"\"\"\n",
        "    # load the train and test data\n",
        "    data_gen = AudioGenerator()\n",
        "    data_gen.load_train_data()\n",
        "    data_gen.load_validation_data()\n",
        "    \n",
        "    # obtain the true transcription and the audio features \n",
        "    if partition == 'validation':\n",
        "        transcr = data_gen.valid_texts[index]\n",
        "        audio_path = data_gen.valid_audio_paths[index]\n",
        "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
        "    elif partition == 'train':\n",
        "        transcr = data_gen.train_texts[index]\n",
        "        audio_path = data_gen.train_audio_paths[index]\n",
        "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
        "    else:\n",
        "        raise Exception('Invalid partition!  Must be \"train\" or \"validation\"')\n",
        "        \n",
        "    # obtain and decode the acoustic model's predictions\n",
        "    input_to_softmax.load_weights(model_path)\n",
        "    prediction = input_to_softmax.predict(np.expand_dims(data_point, axis=0))\n",
        "    output_length = [input_to_softmax.output_length(data_point.shape[0])] \n",
        "    pred_ints = (K.eval(K.ctc_decode(\n",
        "                prediction, output_length)[0][0])+1).flatten().tolist()\n",
        "    \n",
        "    # play the audio file, and display the true and predicted transcriptions\n",
        "    print('-'*80)\n",
        "    Audio(audio_path)\n",
        "    print('True transcription:\\n' + '\\n' + transcr)\n",
        "    print('-'*80)\n",
        "    print('Predicted transcription:\\n' + '\\n' + ''.join(int_sequence_to_text(pred_ints)))\n",
        "    print('-'*80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "qRSsxVtnByg0",
        "outputId": "f0c83b83-3530-49eb-8470-932aad390d21"
      },
      "source": [
        "get_predictions(index=0, \n",
        "                partition='validation',\n",
        "                input_to_softmax=final_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
        "                        filters=200,\n",
        "                        kernel_size=11, \n",
        "                        conv_stride=2,\n",
        "                        conv_border_mode='valid',\n",
        "                        units=200), \n",
        "                model_path='./results/model_end.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "the_input (InputLayer)       [(None, None, 161)]       0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 200)         354400    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d (BatchNormalizati (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d1 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d2 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, None, 29)          5829      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, None, 29)          0         \n",
            "=================================================================\n",
            "Total params: 845,029\n",
            "Trainable params: 843,829\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "True transcription:\n",
            "\n",
            "when i get up i see my skin vague\n",
            "--------------------------------------------------------------------------------\n",
            "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-79c905e619df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mconv_border_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         units=200), \n\u001b[0;32m----> 9\u001b[0;31m                 model_path='./results/model_end.h5')\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-c34179d482d5>\u001b[0m in \u001b[0;36mget_predictions\u001b[0;34m(index, partition, input_to_softmax, model_path)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'True transcription:\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtranscr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted transcription:\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_sequence_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_ints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-01e08588409c>\u001b[0m in \u001b[0;36mint_sequence_to_text\u001b[0;34m(int_sequence)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mint_sequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tggfDAyl50k"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz7okuwWXnKm"
      },
      "source": [
        "#  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ1Px-U6CF-9"
      },
      "source": [
        "train_data = train_corpus[0:2]\n",
        "valid_data = validate_corpus[0:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU56aa__W9ql"
      },
      "source": [
        "import re \n",
        "\n",
        "def featurize(audio_clip, n_fft = 320):\n",
        "    \"\"\" For a given audio clip, calculate the corresponding feature\n",
        "    Params:\n",
        "        audio_clip (str): Path to the audio clip\n",
        "    \"\"\"\n",
        "    return to_spectrogram(audio_clip, n_fft=n_fft)\n",
        "\n",
        "\n",
        "def normalize(feature, eps = 1e-14):\n",
        "    feat_mean = np.mean(feature, axis=0)\n",
        "    feat_std = np.std(feature, axis=0)\n",
        "    return (feature-feat_mean) / (feat_std + eps)\n",
        "\n",
        "\n",
        "def text_to_int_sequence(text):\n",
        "    \"\"\" Convert text to an integer sequence \"\"\"\n",
        "    int_sequence = []\n",
        "    for c in text:\n",
        "\n",
        "        if c == ' ':\n",
        "            ch = char_map['<SPACE>']\n",
        "        else:\n",
        "            ch = char_map[c]\n",
        "        int_sequence.append(ch)\n",
        "    return int_sequence\n",
        "\n",
        "\n",
        "def data_for_fit(corpus):\n",
        "\n",
        "    \"\"\"Obtain a dataset for partition\n",
        "    \"\"\"\n",
        "    audio_paths = list()\n",
        "    texts = list()\n",
        "    labels = list()\n",
        "\n",
        "    for item in corpus:\n",
        "        audio_paths.append(item[\"key\"])\n",
        "\n",
        "        audioText = item[\"text\"].lower()\n",
        "        audioText = re.sub('[^a-zA-Z0-9 \\n]', '', audioText)\n",
        "        audioText = audioText.strip()\n",
        "        texts.append(audioText)\n",
        "\n",
        "    features = [normalize(featurize(a)) for a in audio_paths]\n",
        "\n",
        "    for text in texts:\n",
        "        labels.append(text_to_int_sequence(text))\n",
        "\n",
        "    return features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etjIoFTw1CyJ",
        "outputId": "0ea68c52-fe19-49f9-de9f-111a3a24c286"
      },
      "source": [
        "model = final_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
        "                        filters=200,\n",
        "                        kernel_size=11, \n",
        "                        conv_stride=2,\n",
        "                        conv_border_mode='valid',\n",
        "                        units=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "the_input (InputLayer)       [(None, None, 161)]       0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 200)         354400    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d (BatchNormalizati (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d1 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d2 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, None, 29)          5829      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, None, 29)          0         \n",
            "=================================================================\n",
            "Total params: 845,029\n",
            "Trainable params: 843,829\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47R8zor_1S9h"
      },
      "source": [
        "import _pickle as pickle\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import (Input, Lambda)\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint   \n",
        "import os\n",
        "\n",
        "input_to_softmax = model\n",
        "train_json='train_corpus.json'\n",
        "valid_json='valid_corpus.json'\n",
        "minibatch_size=20\n",
        "spectrogram=True\n",
        "mfcc_dim=13\n",
        "optimizer=SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
        "epochs=20\n",
        "verbose=1\n",
        "sort_by_duration=False\n",
        "max_duration=10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlg6r25l5Q3w"
      },
      "source": [
        "audio_gen = AudioGenerator(minibatch_size=minibatch_size, \n",
        "        spectrogram=spectrogram, mfcc_dim=mfcc_dim, max_duration=max_duration,\n",
        "        sort_by_duration=sort_by_duration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qxkk598fnYu"
      },
      "source": [
        "audio_gen.load_train_data(train_json) # Load Metat data from json and save in lists, then fit_train()\n",
        "audio_gen.load_validation_data(valid_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Q3hKEr1LJE"
      },
      "source": [
        "# add CTC loss to the NN specified in input_to_softmax\n",
        "model = add_ctc_loss(input_to_softmax) #Added Ctc loss\n",
        "\n",
        "# CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "R0B5JwEQ1MyF",
        "outputId": "d0d101de-72c1-4d2f-9722-7f404e30a9c4"
      },
      "source": [
        "# Run by Fit\n",
        "\n",
        "epochs = 10\n",
        "verbose = 1\n",
        "pickle_path = 'model_end.pickle' \n",
        "save_model_path ='model_end.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='results/'+save_model_path, verbose=0)\n",
        "train_x, train_y = data_for_fit(train_data)\n",
        "valid_x, valid_y = data_for_fit(valid_data)\n",
        "\n",
        "hist = model.fit(x = train_x,\n",
        "                 y = train_y,\n",
        "                 steps_per_epoch=steps_per_epoch,\n",
        "                 epochs=epochs,\n",
        "                 validation_data=(valid_x, valid_y),\n",
        "                 validation_steps=validation_steps,\n",
        "                 callbacks=[checkpointer], verbose=verbose)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-b68088b75fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                  \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                  callbacks=[checkpointer], verbose=verbose)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1097\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 964\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    965\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     raise RuntimeError(\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'})"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grtRANOQGdWZ",
        "outputId": "3ebcbfc5-5e46-4e59-e930-962cd7fa2ad1"
      },
      "source": [
        "len(feats)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWj1m93RGg2i"
      },
      "source": [
        " a = [\n",
        "      [[1, 2, 3],\n",
        "      [3, 4, 5],\n",
        "      [6, 7, 8]],\n",
        "      [[10, 11, 12],\n",
        "       [13, 14, 15],\n",
        "       [16, 17, 18]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr1XhA3Bbq8e"
      },
      "source": [
        "new_a = np.vstack(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21ZgDCMJbuq4",
        "outputId": "9d1d9e5d-add0-45f1-8bf2-88e47cc4a600"
      },
      "source": [
        "new_a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [ 3,  4,  5],\n",
              "       [ 6,  7,  8],\n",
              "       [10, 11, 12],\n",
              "       [13, 14, 15],\n",
              "       [16, 17, 18]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcEE3McTbvZ6",
        "outputId": "edc0493c-f098-44bd-9097-f9bbb38ea657"
      },
      "source": [
        "new_a_mean = np.mean(new_a, axis=0)\n",
        "print(new_a_mean)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 8.16666667  9.16666667 10.16666667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmEkBqwEbzR_",
        "outputId": "930ada45-6c47-4cb5-a0b4-938a05e476e6"
      },
      "source": [
        "new_a_std = np.std(new_a, axis=0)\n",
        "print(new_a_std)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5.33593686 5.33593686 5.33593686]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWemDuL6bzx3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E046Du0ZcxE6",
        "outputId": "ef1a4551-a2a5-4c81-c4c2-6b0801ebf9d5"
      },
      "source": [
        "normalize(new_a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.34309435, -1.34309435, -1.34309435],\n",
              "       [-0.96827732, -0.96827732, -0.96827732],\n",
              "       [-0.40605178, -0.40605178, -0.40605178],\n",
              "       [ 0.34358228,  0.34358228,  0.34358228],\n",
              "       [ 0.90580782,  0.90580782,  0.90580782],\n",
              "       [ 1.46803336,  1.46803336,  1.46803336]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtcIHZ79czE-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7zYMLRuXofi"
      },
      "source": [
        "## CTC Loss practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxjJhGnNgkJ2",
        "outputId": "878a8d4e-c0c8-46ed-d95c-fe78561df6fe"
      },
      "source": [
        "!pip install python_speech_features"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python_speech_features\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/d1/94c59e20a2631985fbd2124c45177abaa9e0a4eee8ba8a305aa26fc02a8e/python_speech_features-0.6.tar.gz\n",
            "Building wheels for collected packages: python-speech-features\n",
            "  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-speech-features: filename=python_speech_features-0.6-cp37-none-any.whl size=5887 sha256=b7f46faf934803dc0c19cde22ada3d63c7fec6f92d426a2dab579735224f8571\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/42/7c/f60e9d1b40015cd69b213ad90f7c18a9264cd745b9888134be\n",
            "Successfully built python-speech-features\n",
            "Installing collected packages: python-speech-features\n",
            "Successfully installed python-speech-features-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2frb_NPYqHS"
      },
      "source": [
        "#  Compatibility imports\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from python_speech_features import mfcc\n",
        "\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import scipy.io.wavfile as wav\n",
        "import numpy as np"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_wu1JD8Xrnr"
      },
      "source": [
        "# Constants\n",
        "SPACE_TOKEN = '<space>'\n",
        "SPACE_INDEX = 0\n",
        "FIRST_INDEX = ord('a') - 1  # 0 is reserved to space"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKtFGQTeX0la"
      },
      "source": [
        "# Some configs\n",
        "num_features = 13\n",
        "num_units=50 # Number of units in the LSTM cell\n",
        "# Accounting the 0th indice +  space + blank label = 28 characters\n",
        "num_classes = ord('z') - ord('a') + 1 + 1 + 1"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kILjtLUIX6do"
      },
      "source": [
        "# Hyper-parameters\n",
        "num_epochs = 20\n",
        "num_hidden = 50\n",
        "num_layers = 1\n",
        "batch_size = 1\n",
        "initial_learning_rate = 1e-2\n",
        "momentum = 0.9"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13vAqYryX_uP"
      },
      "source": [
        "num_examples = 1\n",
        "num_batches_per_epoch = int(num_examples/batch_size)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO2YnO63YEnM"
      },
      "source": [
        "# Loading the data\n",
        "\n",
        "audio_filename = train_corpus[0][\"key\"]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIjZXeT6YgBN"
      },
      "source": [
        "fs, audio = wav.read(audio_filename) #fs: sample rate, audio: data"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmVpall-YmRS",
        "outputId": "2ec3c9da-9b63-4335-b63a-ad188e10a8da"
      },
      "source": [
        "inputs = mfcc(audio, samplerate=fs)\n",
        "print(inputs.shape)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:frame length (1200) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(699, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs1XakOOYwCr",
        "outputId": "316ee03b-b1e3-445c-9e72-06f44c2169bf"
      },
      "source": [
        "# Tranform in 3D array\n",
        "train_inputs = np.asarray(inputs[np.newaxis, :])\n",
        "print(train_inputs.shape)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 699, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AipWGGreYwd9",
        "outputId": "84304c85-9314-4797-eee3-60233ae429f8"
      },
      "source": [
        "# Normalize inputs\n",
        "train_inputs = (train_inputs - np.mean(train_inputs))/np.std(train_inputs)\n",
        "# Sequance Length\n",
        "train_seq_len = [train_inputs.shape[1]]\n",
        "print(train_seq_len)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[699]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-wdj493hN12",
        "outputId": "e986b819-c224-412d-cc55-f03c11cf857f"
      },
      "source": [
        "line = train_corpus[0][\"text\"]\n",
        "\n",
        "# Get only the words between [a-z] and replace period for none\n",
        "original = ' '.join(line.strip().lower().split(' ')[2:]).replace('.', '')\n",
        "targets = original.replace(' ', '  ')\n",
        "targets = targets.split(' ')\n",
        "print(targets)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', '', 'sprained', '', 'my', '', 'ankle', '', 'it', '', 'really', '', 'hurts']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDeNZgN6hg1H",
        "outputId": "810e87db-aafa-4828-e5b6-c3f0fb302403"
      },
      "source": [
        "# Adding blank label\n",
        "targets = np.hstack([SPACE_TOKEN if x == '' else list(x) for x in targets])\n",
        "print(targets)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i' '<space>' 's' 'p' 'r' 'a' 'i' 'n' 'e' 'd' '<space>' 'm' 'y' '<space>'\n",
            " 'a' 'n' 'k' 'l' 'e' '<space>' 'i' 't' '<space>' 'r' 'e' 'a' 'l' 'l' 'y'\n",
            " '<space>' 'h' 'u' 'r' 't' 's']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WescP8QhhhVD",
        "outputId": "3fb5a5e8-3101-4ae8-b354-64055692970d"
      },
      "source": [
        "# Transform char into index\n",
        "targets = np.asarray([SPACE_INDEX if x == SPACE_TOKEN else ord(x) - FIRST_INDEX for x in targets])\n",
        "print(targets)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 9  0 19 16 18  1  9 14  5  4  0 13 25  0  1 14 11 12  5  0  9 20  0 18\n",
            "  5  1 12 12 25  0  8 21 18 20 19]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HvWvaBWiK7P"
      },
      "source": [
        "# Creating sparse representation to feed the placeholder\n",
        "train_targets = sparse_tuple_from([targets])\n"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtvWs8BRi1QM"
      },
      "source": [
        "# We don't have a validation dataset :(\n",
        "val_inputs, val_targets, val_seq_len = train_inputs, train_targets, \\\n",
        "                                       train_seq_len"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU5dbd4Php-h"
      },
      "source": [
        "graph = tf.Graph()"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LHPeY6VhtHs",
        "outputId": "68888459-3a45-4cdf-e8e9-6617bb3f27f1"
      },
      "source": [
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "\n",
        "with graph.as_default():\n",
        "    # e.g: log filter bank or MFCC features\n",
        "    # Has size [batch_size, max_stepsize, num_features], but the\n",
        "    # batch_size and max_stepsize can vary along each step\n",
        "    inputs = tf.placeholder(tf.float32, [None, None, num_features]) #num_features = 13 for mfcc\n",
        "    print(\"Inputs: \", inputs)\n",
        "    print(\"\")\n",
        "\n",
        "    # Here we use sparse_placeholder that will generate a\n",
        "    # SparseTensor required by ctc_loss op.\n",
        "    targets = tf.sparse_placeholder(tf.int32)\n",
        "    print(\"Targets: \", targets)\n",
        "    print(\"\")\n",
        "\n",
        "    # 1d array of size [batch_size]\n",
        "    seq_len = tf.placeholder(tf.int32, [None])\n",
        "    print(\"Seq len: \", seq_len)\n",
        "    print(\"\")\n",
        "\n",
        "    # Defining the cell\n",
        "    # Can be:\n",
        "    #   tf.nn.rnn_cell.RNNCell\n",
        "    #   tf.nn.rnn_cell.GRUCell \n",
        "    cells = []\n",
        "    for _ in range(num_layers):\n",
        "        cell = tf.nn.rnn_cell.LSTMCell(num_units)  # Or LSTMCell(num_units)\n",
        "        cells.append(cell)\n",
        "    stack = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
        "\n",
        "    # The second output is the last state and we will no use that\n",
        "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
        "\n",
        "    shape = tf.shape(inputs)\n",
        "    batch_s, max_timesteps = shape[0], shape[1]\n",
        "\n",
        "    # Reshaping to apply the same weights over the timesteps\n",
        "    outputs = tf.reshape(outputs, [-1, num_hidden])\n",
        "\n",
        "    # Truncated normal with mean 0 and stdev=0.1\n",
        "    # Tip: Try another initialization\n",
        "    # see https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.layers.html#initializers\n",
        "    W = tf.Variable(tf.truncated_normal([num_hidden,\n",
        "                                         num_classes],\n",
        "                                        stddev=0.1))\n",
        "    print(\"W: \", W)\n",
        "    print(\"\")\n",
        "    # Zero initialization\n",
        "    # Tip: Is tf.zeros_initializer the same?\n",
        "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
        "    print(\"b: \", b)\n",
        "    print(\"\")\n",
        "\n",
        "    # Doing the affine projection\n",
        "    logits = tf.matmul(outputs, W) + b\n",
        "    print(\"Outputs: \", outputs)\n",
        "    print(\"\")\n",
        "    print(\"Logits: \", logits)\n",
        "    print(\"\")\n",
        "\n",
        "    # Reshaping back to the original shape\n",
        "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
        "    print(\"Logits after Reshape: \", logits)\n",
        "\n",
        "\n",
        "    # Time major\n",
        "    logits = tf.transpose(logits, (1, 0, 2))\n",
        "    print(\"Logits after Transpose: \", logits)\n",
        "    print(\"\")\n",
        "\n",
        "    loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
        "    cost = tf.reduce_mean(loss)\n",
        "\n",
        "    optimizer = tf.train.MomentumOptimizer(initial_learning_rate,\n",
        "                                           0.9).minimize(cost)\n",
        "\n",
        "    # Option 2: tf.nn.ctc_beam_search_decoder\n",
        "    # (it's slower but you'll get better results)\n",
        "    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
        "\n",
        "    # Inaccuracy: label error rate\n",
        "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
        "                                          targets))\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs:  Tensor(\"Placeholder:0\", shape=(?, ?, 13), dtype=float32)\n",
            "\n",
            "Targets:  SparseTensor(indices=Tensor(\"Placeholder_3:0\", shape=(?, ?), dtype=int64), values=Tensor(\"Placeholder_2:0\", shape=(?,), dtype=int32), dense_shape=Tensor(\"Placeholder_1:0\", shape=(?,), dtype=int64))\n",
            "\n",
            "Seq len:  Tensor(\"Placeholder_4:0\", shape=(?,), dtype=int32)\n",
            "\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:903: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "W:  <tf.Variable 'Variable:0' shape=(50, 28) dtype=float32_ref>\n",
            "\n",
            "b:  <tf.Variable 'Variable_1:0' shape=(28,) dtype=float32_ref>\n",
            "\n",
            "Outputs:  Tensor(\"Reshape:0\", shape=(?, 50), dtype=float32)\n",
            "\n",
            "Logits:  Tensor(\"add:0\", shape=(?, 28), dtype=float32)\n",
            "\n",
            "Logits after Reshape:  Tensor(\"Reshape_1:0\", shape=(?, ?, 28), dtype=float32)\n",
            "Logits after Transpose:  Tensor(\"transpose:0\", shape=(?, ?, 28), dtype=float32)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqilhi0dk4F9",
        "outputId": "ded65476-305e-4d9e-8eca-fb1469b403f3"
      },
      "source": [
        "num_epochs = 40\n",
        "with tf.Session(graph=graph) as session:\n",
        "    # Initializate the weights and biases\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "\n",
        "    for curr_epoch in range(num_epochs):\n",
        "        train_cost = train_ler = 0\n",
        "        start = time.time()\n",
        "\n",
        "        for batch in range(num_batches_per_epoch):\n",
        "\n",
        "            feed = {inputs: train_inputs,\n",
        "                    targets: train_targets,\n",
        "                    seq_len: train_seq_len}\n",
        "\n",
        "            batch_cost, _ = session.run([cost, optimizer], feed)\n",
        "            train_cost += batch_cost*batch_size\n",
        "            train_ler += session.run(ler, feed_dict=feed)*batch_size\n",
        "\n",
        "        train_cost /= num_examples\n",
        "        train_ler /= num_examples\n",
        "\n",
        "        val_feed = {inputs: val_inputs,\n",
        "                    targets: val_targets,\n",
        "                    seq_len: val_seq_len}\n",
        "\n",
        "        val_cost, val_ler = session.run([cost, ler], feed_dict=val_feed)\n",
        "\n",
        "        log = \"Epoch {}/{}, train_cost = {:.3f}, train_ler = {:.3f}, val_cost = {:.3f}, val_ler = {:.3f}, time = {:.3f}\"\n",
        "        print(log.format(curr_epoch+1, num_epochs, train_cost, train_ler,\n",
        "                         val_cost, val_ler, time.time() - start))\n",
        "    # Decoding\n",
        "    d = session.run(decoded[0], feed_dict=feed)\n",
        "    str_decoded = ''.join([chr(x) for x in np.asarray(d[1]) + FIRST_INDEX])\n",
        "    # Replacing blank label to none\n",
        "    str_decoded = str_decoded.replace(chr(ord('z') + 1), '')\n",
        "    # Replacing space label to space\n",
        "    str_decoded = str_decoded.replace(chr(ord('a') - 1), ' ')\n",
        "\n",
        "    print('Original:\\n%s' % original)\n",
        "    print('Decoded:\\n%s' % str_decoded)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40, train_cost = 2061.447, train_ler = 0.971, val_cost = 235.062, val_ler = 0.971, time = 1.110\n",
            "Epoch 2/40, train_cost = 235.062, train_ler = 0.971, val_cost = 677.521, val_ler = 0.971, time = 0.964\n",
            "Epoch 3/40, train_cost = 677.521, train_ler = 0.971, val_cost = 616.778, val_ler = 0.971, time = 0.966\n",
            "Epoch 4/40, train_cost = 616.778, train_ler = 0.743, val_cost = 234.425, val_ler = 0.743, time = 1.016\n",
            "Epoch 5/40, train_cost = 234.425, train_ler = 0.743, val_cost = 245.300, val_ler = 0.743, time = 1.032\n",
            "Epoch 6/40, train_cost = 245.300, train_ler = 0.971, val_cost = 352.588, val_ler = 0.971, time = 1.025\n",
            "Epoch 7/40, train_cost = 352.588, train_ler = 0.971, val_cost = 348.542, val_ler = 0.971, time = 1.018\n",
            "Epoch 8/40, train_cost = 348.542, train_ler = 0.971, val_cost = 214.362, val_ler = 0.971, time = 1.063\n",
            "Epoch 9/40, train_cost = 214.362, train_ler = 1.057, val_cost = 267.700, val_ler = 1.057, time = 1.041\n",
            "Epoch 10/40, train_cost = 267.700, train_ler = 0.743, val_cost = 176.088, val_ler = 0.743, time = 1.023\n",
            "Epoch 11/40, train_cost = 176.088, train_ler = 0.800, val_cost = 173.106, val_ler = 0.800, time = 1.022\n",
            "Epoch 12/40, train_cost = 173.106, train_ler = 0.857, val_cost = 181.677, val_ler = 0.857, time = 1.049\n",
            "Epoch 13/40, train_cost = 181.677, train_ler = 0.743, val_cost = 156.869, val_ler = 0.743, time = 0.999\n",
            "Epoch 14/40, train_cost = 156.869, train_ler = 0.743, val_cost = 160.621, val_ler = 0.743, time = 0.940\n",
            "Epoch 15/40, train_cost = 160.621, train_ler = 0.857, val_cost = 142.607, val_ler = 0.857, time = 0.965\n",
            "Epoch 16/40, train_cost = 142.607, train_ler = 0.886, val_cost = 131.261, val_ler = 0.886, time = 0.946\n",
            "Epoch 17/40, train_cost = 131.261, train_ler = 0.914, val_cost = 195.729, val_ler = 0.914, time = 0.945\n",
            "Epoch 18/40, train_cost = 195.729, train_ler = 0.914, val_cost = 147.647, val_ler = 0.914, time = 0.944\n",
            "Epoch 19/40, train_cost = 147.647, train_ler = 0.886, val_cost = 130.670, val_ler = 0.886, time = 0.920\n",
            "Epoch 20/40, train_cost = 130.670, train_ler = 0.886, val_cost = 161.712, val_ler = 0.886, time = 0.928\n",
            "Epoch 21/40, train_cost = 161.712, train_ler = 0.800, val_cost = 127.599, val_ler = 0.800, time = 0.921\n",
            "Epoch 22/40, train_cost = 127.599, train_ler = 0.714, val_cost = 123.730, val_ler = 0.714, time = 0.926\n",
            "Epoch 23/40, train_cost = 123.730, train_ler = 0.771, val_cost = 116.866, val_ler = 0.771, time = 0.965\n",
            "Epoch 24/40, train_cost = 116.866, train_ler = 0.771, val_cost = 118.676, val_ler = 0.771, time = 0.949\n",
            "Epoch 25/40, train_cost = 118.676, train_ler = 0.771, val_cost = 123.749, val_ler = 0.771, time = 0.926\n",
            "Epoch 26/40, train_cost = 123.749, train_ler = 0.800, val_cost = 113.743, val_ler = 0.800, time = 0.923\n",
            "Epoch 27/40, train_cost = 113.743, train_ler = 0.743, val_cost = 110.386, val_ler = 0.743, time = 0.942\n",
            "Epoch 28/40, train_cost = 110.386, train_ler = 0.743, val_cost = 111.699, val_ler = 0.743, time = 0.953\n",
            "Epoch 29/40, train_cost = 111.699, train_ler = 0.771, val_cost = 111.024, val_ler = 0.771, time = 0.942\n",
            "Epoch 30/40, train_cost = 111.024, train_ler = 0.714, val_cost = 105.401, val_ler = 0.714, time = 0.959\n",
            "Epoch 31/40, train_cost = 105.401, train_ler = 0.743, val_cost = 102.519, val_ler = 0.743, time = 0.928\n",
            "Epoch 32/40, train_cost = 102.519, train_ler = 0.714, val_cost = 104.145, val_ler = 0.714, time = 0.952\n",
            "Epoch 33/40, train_cost = 104.145, train_ler = 0.686, val_cost = 101.587, val_ler = 0.686, time = 0.936\n",
            "Epoch 34/40, train_cost = 101.587, train_ler = 0.686, val_cost = 98.280, val_ler = 0.686, time = 0.954\n",
            "Epoch 35/40, train_cost = 98.280, train_ler = 0.686, val_cost = 97.998, val_ler = 0.686, time = 0.934\n",
            "Epoch 36/40, train_cost = 97.998, train_ler = 0.771, val_cost = 94.940, val_ler = 0.771, time = 0.933\n",
            "Epoch 37/40, train_cost = 94.940, train_ler = 0.771, val_cost = 92.381, val_ler = 0.771, time = 0.935\n",
            "Epoch 38/40, train_cost = 92.381, train_ler = 0.771, val_cost = 91.779, val_ler = 0.771, time = 0.939\n",
            "Epoch 39/40, train_cost = 91.779, train_ler = 0.771, val_cost = 91.194, val_ler = 0.771, time = 0.929\n",
            "Epoch 40/40, train_cost = 91.194, train_ler = 0.771, val_cost = 89.050, val_ler = 0.771, time = 0.958\n",
            "Original:\n",
            "i sprained my ankle it really hurts\n",
            "Decoded:\n",
            " ae ane \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe_01msAmwBY",
        "outputId": "21bf3b13-6c8e-410d-b725-1db40efe48a3"
      },
      "source": [
        "targets"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9,  0, 19, 16, 18,  1,  9, 14,  5,  4,  0, 13, 25,  0,  1, 14, 11,\n",
              "       12,  5,  0,  9, 20,  0, 18,  5,  1, 12, 12, 25,  0,  8, 21, 18, 20,\n",
              "       19])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR-9NyaQolNU"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_input = to_spectrogram(audio_filename)\n",
        "train_target = train_targets\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Le2jmbDAYBE",
        "outputId": "63861199-d638-484b-a274-e9fd001c8ca4"
      },
      "source": [
        "a = "
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0],\n",
              "       [ 0,  1],\n",
              "       [ 0,  2],\n",
              "       [ 0,  3],\n",
              "       [ 0,  4],\n",
              "       [ 0,  5],\n",
              "       [ 0,  6],\n",
              "       [ 0,  7],\n",
              "       [ 0,  8],\n",
              "       [ 0,  9],\n",
              "       [ 0, 10],\n",
              "       [ 0, 11],\n",
              "       [ 0, 12],\n",
              "       [ 0, 13],\n",
              "       [ 0, 14],\n",
              "       [ 0, 15],\n",
              "       [ 0, 16],\n",
              "       [ 0, 17],\n",
              "       [ 0, 18],\n",
              "       [ 0, 19],\n",
              "       [ 0, 20],\n",
              "       [ 0, 21],\n",
              "       [ 0, 22],\n",
              "       [ 0, 23],\n",
              "       [ 0, 24],\n",
              "       [ 0, 25],\n",
              "       [ 0, 26],\n",
              "       [ 0, 27],\n",
              "       [ 0, 28],\n",
              "       [ 0, 29],\n",
              "       [ 0, 30],\n",
              "       [ 0, 31],\n",
              "       [ 0, 32],\n",
              "       [ 0, 33],\n",
              "       [ 0, 34]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDRC1KNY9af-",
        "outputId": "dc7cfd36-20a5-4924-b899-714d730542ba"
      },
      "source": [
        "model = final_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
        "                        filters=200,\n",
        "                        kernel_size=11, \n",
        "                        conv_stride=2,\n",
        "                        conv_border_mode='valid',\n",
        "                        units=200)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "the_input (InputLayer)       [(None, None, 161)]       0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 200)         354400    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d (BatchNormalizati (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d1 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d2 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, None, 29)          5829      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, None, 29)          0         \n",
            "=================================================================\n",
            "Total params: 845,029\n",
            "Trainable params: 843,829\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU0Cc_9-qSXA"
      },
      "source": [
        "import _pickle as pickle\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import (Input, Lambda)\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint   \n",
        "import os\n",
        "\n",
        "input_to_softmax = model\n",
        "train_json='train_corpus.json'\n",
        "valid_json='valid_corpus.json'\n",
        "minibatch_size=20\n",
        "spectrogram=True\n",
        "mfcc_dim=13\n",
        "optimizer=SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
        "epochs=20\n",
        "verbose=1\n",
        "sort_by_duration=False\n",
        "max_duration=10.0"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "GG8NJZbT054h",
        "outputId": "9c99a068-8226-4f9b-9131-3e6e6b7833e9"
      },
      "source": [
        "model = final_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
        "                        filters=200,\n",
        "                        kernel_size=11, \n",
        "                        conv_stride=2,\n",
        "                        conv_border_mode='valid',\n",
        "                        units=200)\n",
        "\n",
        "# add CTC loss to the NN specified in input_to_softmax\n",
        "model = add_ctc_loss(input_to_softmax) #Added Ctc loss\n",
        "\n",
        "# CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_10 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_10 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "the_input (InputLayer)       [(None, None, 161)]       0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 200)         354400    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d (BatchNormalizati (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_10 (GRU)                 (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d1 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_11 (GRU)                 (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d2 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, None, 29)          5829      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, None, 29)          0         \n",
            "=================================================================\n",
            "Total params: 845,029\n",
            "Trainable params: 843,829\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-153-218caff0245e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# add CTC loss to the NN specified in input_to_softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_ctc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_to_softmax\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Added Ctc loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-3c40d9e71078>\u001b[0m in \u001b[0;36madd_ctc_loss\u001b[0;34m(input_to_softmax)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlabel_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moutput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_to_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# CTC loss is implemented in a lambda layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'output_length'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo818BYD06Pu",
        "outputId": "3bfb066d-14cc-428b-93c5-24aa74e164db"
      },
      "source": [
        "shape[1]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'strided_slice_1:0' shape=() dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6OSimH407lv",
        "outputId": "102759d8-ccfd-45c8-d892-643614b799b3"
      },
      "source": [
        "inputs"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Placeholder_2:0' shape=(?, ?, 13) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCLf6N4W3B_-",
        "outputId": "5c5cdca8-cbce-456a-ecba-67a3eea010c9"
      },
      "source": [
        "targets"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fdd2a368810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K11BZLG_3CeQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}