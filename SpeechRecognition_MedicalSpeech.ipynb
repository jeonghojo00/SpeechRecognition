{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpeechRecognition_MedicalSpeech",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1v9FcT4_pk1V2TVjJJ2bo32MhKbgHqSD5",
      "authorship_tag": "ABX9TyMgER4Ue2MfNesJ6B1j5tFg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonghojo00/SpeechRecognition/blob/main/SpeechRecognition_MedicalSpeech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWeYRH8U9tWi"
      },
      "source": [
        "# Target 1: Convert Wav file into Text\n",
        " Target 2: Analyze symptoms with text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hba77M7HLU_V"
      },
      "source": [
        "## Step 0. Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjyxXokk906a",
        "outputId": "2be337fe-f845-4f50-e03f-a64e48aad343"
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ki8pALfQDcH"
      },
      "source": [
        "# Change current directory\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/SpeechRecognition/MedicalSpeech')\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GVfCfoJOkRN",
        "outputId": "d2017db2-274c-4035-a3fa-c888489fcbd4"
      },
      "source": [
        "# Import Overview csv file\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#Analyze Data\n",
        "def explore_data(df):\n",
        "    print(f\"The data contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "    print('\\n')\n",
        "    print('Dataset columns:',df.columns)\n",
        "    print('\\n')\n",
        "    print(df.info())\n",
        "\n",
        "filename = \"overview-of-recordings.csv\"\n",
        "overview = pd.read_csv(filename)\n",
        "explore_data(overview)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The data contains 6661 rows and 13 columns.\n",
            "\n",
            "\n",
            "Dataset columns: Index(['audio_clipping', 'audio_clipping:confidence',\n",
            "       'background_noise_audible', 'background_noise_audible:confidence',\n",
            "       'overall_quality_of_the_audio', 'quiet_speaker',\n",
            "       'quiet_speaker:confidence', 'speaker_id', 'file_download', 'file_name',\n",
            "       'phrase', 'prompt', 'writer_id'],\n",
            "      dtype='object')\n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6661 entries, 0 to 6660\n",
            "Data columns (total 13 columns):\n",
            " #   Column                               Non-Null Count  Dtype  \n",
            "---  ------                               --------------  -----  \n",
            " 0   audio_clipping                       6661 non-null   object \n",
            " 1   audio_clipping:confidence            6661 non-null   float64\n",
            " 2   background_noise_audible             6661 non-null   object \n",
            " 3   background_noise_audible:confidence  6661 non-null   float64\n",
            " 4   overall_quality_of_the_audio         6661 non-null   float64\n",
            " 5   quiet_speaker                        6661 non-null   object \n",
            " 6   quiet_speaker:confidence             6661 non-null   float64\n",
            " 7   speaker_id                           6661 non-null   int64  \n",
            " 8   file_download                        6661 non-null   object \n",
            " 9   file_name                            6661 non-null   object \n",
            " 10  phrase                               6661 non-null   object \n",
            " 11  prompt                               6661 non-null   object \n",
            " 12  writer_id                            6661 non-null   int64  \n",
            "dtypes: float64(4), int64(2), object(7)\n",
            "memory usage: 676.6+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvQTzFTFM-au"
      },
      "source": [
        "## Step 1. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmT1Dx8xV7xm"
      },
      "source": [
        "##### Preprocessing includes following actions:\n",
        "1. Load audio file\n",
        "2. Load csv file\n",
        "3. Convert audio file into spectrogram feature, mfcc feature\n",
        "4/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE1GoZ3Gc3aO"
      },
      "source": [
        "### Step 1-1. Import train, validate, and test corpus json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhAwKR_cQZO7"
      },
      "source": [
        "import json\n",
        "\n",
        "f = open('train_corpus.json',)\n",
        "train_corpus = json.load(f)\n",
        "\n",
        "f = open('valid_corpus.json',)\n",
        "validate_corpus = json.load(f)\n",
        "\n",
        "f = open('test_corpus.json',)\n",
        "test_corpus = json.load(f)\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOP8yKkAXhqm"
      },
      "source": [
        "### Step 1-2. Define functions for various wavelengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yav0lvdjfjjz"
      },
      "source": [
        "# Transform wav into signal and rate\n",
        "import librosa, librosa.display\n",
        "\n",
        "signal, sr = librosa.load(train_corpus[0][\"key\"], sr=22050)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "3VOKuotmhNIg",
        "outputId": "f04dccce-db40-4ebb-986c-625ef4790476"
      },
      "source": [
        "# WaveForm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "librosa.display.waveplot(signal, sr=sr)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7VT5nLmng0"
      },
      "source": [
        "import librosa, librosa.display\n",
        "import numpy as np\n",
        "\n",
        "def to_spectrogram(file_path, n_fft=2048, sr=22050, plot=False):\n",
        "    # Transform wav into signal and rate\n",
        "    signal, sr = librosa.load(file_path, sr=sr)\n",
        "\n",
        "    # Spectrogram\n",
        "\n",
        "    #n_fft: Number of samples per fft\n",
        "    hop_length = 512 # Amount we shift to the right\n",
        "\n",
        "    stft = librosa.core.stft(signal, hop_length=hop_length, n_fft = n_fft)\n",
        "    spectrogram = np.abs(stft)\n",
        "\n",
        "    log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
        "    \n",
        "    if plot:\n",
        "        # Plot the result\n",
        "        librosa.display.specshow(log_spectrogram, sr=sr, hop_length=hop_length)\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "    \n",
        "    tr_log_spectrogram = np.transpose(log_spectrogram)\n",
        "    return tr_log_spectrogram"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjOdHcAvmsm9"
      },
      "source": [
        "import numpy as np\n",
        "def normalize(feature, eps=1e-14):\n",
        "        \"\"\" Center a feature using the mean and std\n",
        "        Params:\n",
        "            feature (numpy.ndarray): Feature to normalize\n",
        "        \"\"\"\n",
        "        feats_mean = np.mean(log_spectrogram, axis=0)\n",
        "        feats_std = np.std(log_spectrogram, axis=0)\n",
        "        return (feature - feats_mean) / (feats_std + eps)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sMD3S1VWYrz"
      },
      "source": [
        "def to_mfcc(file_path, sr=22050, plot=False):\n",
        "    # MFCCs\n",
        "    signal, sr = librosa.load(train_corpus[0][\"key\"], sr=sr)\n",
        "    MFCCs = librosa.feature.mfcc(signal, n_fft=n_fft, hop_length=hop_length, n_mfcc=13)\n",
        "\n",
        "    if plot:\n",
        "        librosa.display.specshow(MFCCs, sr=sr, hop_length=hop_length)\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Pp2-Lejd7j"
      },
      "source": [
        "def to_spectrum(file_path, sr=22050, plot=False):\n",
        "    # Spectrum\n",
        "    # Transform wav into signal and rate\n",
        "    signal, sr = librosa.load(train_corpus[0][\"key\"], sr=sr)\n",
        "\n",
        "    fft = np.fft.fft(signal)\n",
        "    magnitude = np.abs(fft)\n",
        "    frequency = np.linspace(0, sr, len(magnitude))\n",
        "\n",
        "    # Because once we pass over the half, we repeat the same information. we use only the first half\n",
        "    left_frequency = frequency[:int(len(frequency)/2)]\n",
        "    left_magnitude = magnitude[:int(len(magnitude)/2)]\n",
        "\n",
        "    if plot:\n",
        "\n",
        "        plt.plot(left_frequency, left_magnitude)\n",
        "        plt.xlabel(\"Frequency\")\n",
        "        plt.ylabel(\"Magnitude\")\n",
        "        plt.show()\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ymHLA2Phq6"
      },
      "source": [
        "### Step 1.3. Prepare Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAJTwd5jk65g"
      },
      "source": [
        "SPACE_TOKEN = '<space>'\n",
        "SPACE_INDEX = 0\n",
        "FIRST_INDEX = ord('a') - 1  # 0 is reserved to space\n",
        "\n",
        "def convert_inputs_to_ctc_format(audio_path, feature, text, samplerate=22050):\n",
        "    # Convert auido file into audio feature in 3D\n",
        "    ## Load audio file into selected feature\n",
        "    if feature == \"spectrogram\":\n",
        "        inputs = to_spectrogram(audio_path, sr=samplerate)\n",
        "    elif feature == \"mfcc\":\n",
        "        inputs = to_mfcc(audio_path, sr=samplerate)\n",
        "    elif feature == \"spectrum\":\n",
        "        inputs = to_spectrum(audio_path, sr=samplerate)\n",
        "    \n",
        "    ## Change inputs into 3D\n",
        "    train_inputs = np.asarray(inputs[np.newaxis, :])\n",
        "    train_inputs = (train_inputs - np.mean(train_inputs)) / np.std(train_inputs)\n",
        "    train_seq_len = [train_inputs.shape[1]]\n",
        "\n",
        "    # Get only the words between [a-z] and replace period for none\n",
        "    original = ' '.join(text.strip().lower().split(' '))\\\n",
        "                  .replace('.', '').replace('?', '').replace(',','').replace(\"'\", '').replace('!', '').replace('-', '')\n",
        "\n",
        "    targets = original.replace(' ', '  ')\n",
        "    targets = targets.split(' ')\n",
        "\n",
        "    # Adding blank label\n",
        "    targets = np.hstack([SPACE_TOKEN if x == '' else list(x) for x in targets])\n",
        "\n",
        "    # Transform char into index\n",
        "    targets = np.asarray([SPACE_INDEX if x == SPACE_TOKEN else ord(x) - FIRST_INDEX\n",
        "                          for x in targets])\n",
        "\n",
        "    return train_inputs, train_seq_len, targets, original\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FguPmjK74mOr"
      },
      "source": [
        "def pad_sequences(sequences, maxlen=None, dtype=np.float32,\n",
        "                  padding='post', truncating='post', value=0.):\n",
        "    '''Pads each sequence to the same length: the length of the longest\n",
        "    sequence.\n",
        "        If maxlen is provided, any sequence longer than maxlen is truncated to\n",
        "        maxlen. Truncation happens off either the beginning or the end\n",
        "        (default) of the sequence. Supports post-padding (default) and\n",
        "        pre-padding.\n",
        "        Args:\n",
        "            sequences: list of lists where each element is a sequence\n",
        "            maxlen: int, maximum length\n",
        "            dtype: type to cast the resulting sequence.\n",
        "            padding: 'pre' or 'post', pad either before or after each sequence.\n",
        "            truncating: 'pre' or 'post', remove values from sequences larger\n",
        "            than maxlen either in the beginning or in the end of the sequence\n",
        "            value: float, value to pad the sequences to the desired value.\n",
        "        Returns\n",
        "            x: numpy array with dimensions (number_of_sequences, maxlen)\n",
        "            lengths: numpy array with the original sequence lengths\n",
        "    '''\n",
        "    lengths = np.asarray([len(s) for s in sequences], dtype=np.int64)\n",
        "\n",
        "    nb_samples = len(sequences)\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "\n",
        "    # take the sample shape from the first non empty sequence\n",
        "    # checking for consistency in the main loop below.\n",
        "    sample_shape = tuple()\n",
        "    for s in sequences:\n",
        "        if len(s) > 0:\n",
        "            sample_shape = np.asarray(s).shape[1:]\n",
        "            break\n",
        "\n",
        "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if len(s) == 0:\n",
        "            continue  # empty list was found\n",
        "        if truncating == 'pre':\n",
        "            trunc = s[-maxlen:]\n",
        "        elif truncating == 'post':\n",
        "            trunc = s[:maxlen]\n",
        "        else:\n",
        "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
        "\n",
        "        # check `trunc` has expected shape\n",
        "        trunc = np.asarray(trunc, dtype=dtype)\n",
        "        if trunc.shape[1:] != sample_shape:\n",
        "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
        "                             (trunc.shape[1:], idx, sample_shape))\n",
        "\n",
        "        if padding == 'post':\n",
        "            x[idx, :len(trunc)] = trunc\n",
        "        elif padding == 'pre':\n",
        "            x[idx, -len(trunc):] = trunc\n",
        "        else:\n",
        "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
        "    return x, lengths"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E0tSwJVwqcV"
      },
      "source": [
        "SPACE_TOKEN = '<space>'\n",
        "SPACE_INDEX = 0\n",
        "FIRST_INDEX = ord('a') - 1  # 0 is reserved to space\n",
        "\n",
        "def decode_text(y_predict, original):\n",
        "    # Decode list of integer into the text\n",
        "    aligned_original_string = ''\n",
        "    aligned_decoded_string = ''\n",
        "\n",
        "    str_decoded = ''.join([chr(x) for x in np.asarray(y_predict) + FIRST_INDEX])\n",
        "\n",
        "    str_decoded = str_decoded.replace(chr(ord('z') + 1), '')\n",
        "    # Replacing space label to space\n",
        "    str_decoded = str_decoded.replace(chr(ord('a') - 1), ' ')\n",
        "    maxlen = max(len(original), len(str_decoded))\n",
        "    aligned_original_string += str(original).ljust(maxlen)\n",
        "    aligned_decoded_string += str(str_decoded).ljust(maxlen)\n",
        "\n",
        "    print('- Original: %s' % (aligned_original_string))\n",
        "    print('- Decoded : %s' % (aligned_decoded_string))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0af_8hhXqLaa"
      },
      "source": [
        "def sparse_tuple_from(sequences, dtype=np.int32):\n",
        "    \"\"\"Create a sparse representention of x.\n",
        "    Args:\n",
        "        sequences: a list of lists of type dtype where each element is a sequence\n",
        "    Returns:\n",
        "        A tuple with (indices, values, shape)\n",
        "    \"\"\"\n",
        "    indices = []\n",
        "    values = []\n",
        "\n",
        "    for n, seq in enumerate(sequences):\n",
        "        indices.extend(zip([n] * len(seq), range(len(seq))))\n",
        "        values.extend(seq)\n",
        "\n",
        "    indices = np.asarray(indices, dtype=np.int64)\n",
        "    values = np.asarray(values, dtype=dtype)\n",
        "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n",
        "\n",
        "    return indices, values, shape"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHrkJXgijJA5"
      },
      "source": [
        "# Train Dataset\n",
        "feats = list()\n",
        "texts = list()\n",
        "durations = list()\n",
        "originals = list()\n",
        "\n",
        "for data in train_corpus:\n",
        "    durations.append(data['duration'])\n",
        "    train_inputs, train_seq_len, converted_text, original_text = convert_inputs_to_ctc_format(data['key'], \"spectrogram\", data['text'], samplerate=22050)\n",
        "    feats.append(train_inputs)\n",
        "    texts.append(converted_text)\n",
        "    originals.append(original_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3iglQO0nvhZ"
      },
      "source": [
        "padded_texts, org_seq = pad_sequences(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg-W4R-TZ98S"
      },
      "source": [
        "## Step 2. Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OnOHS8ylaBqz",
        "outputId": "c6164929-ff09-4188-ad11-b43ae79fd7f4"
      },
      "source": [
        "'''\n",
        "Recurrent Neural Network works weell in sequential data better than Feed Forward Neural Network\n",
        "Because speech is sequential data, we use Recurrent Neural Network\n",
        "'''\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nRecurrent Neural Network works weell in sequential data better than Feed Forward Neural Network\\nBecause speech is sequential data, we use Recurrent Neural Network\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWHX-M1siGQd"
      },
      "source": [
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "def add_ctc_loss(input_to_softmax):\n",
        "    the_labels = Input(name='the_labels', shape=(None,), dtype='float32')\n",
        "    input_lengths = Input(name='input_length', shape=(1,), dtype='int64')\n",
        "    label_lengths = Input(name='label_length', shape=(1,), dtype='int64')\n",
        "    output_lengths = Lambda(input_to_softmax.output_length)(input_lengths)\n",
        "    # CTC loss is implemented in a lambda layer\n",
        "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
        "        [input_to_softmax.output, the_labels, output_lengths, label_lengths])\n",
        "    model = Model(\n",
        "        inputs=[input_to_softmax.input, the_labels, input_lengths, label_lengths], \n",
        "        outputs=loss_out)\n",
        "    return model\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuE5LnXgav8O"
      },
      "source": [
        "RNG_SEED = 123\n",
        "\n",
        "\n",
        "class AudioGenerator():\n",
        "    def __init__(self, step=10, window=20, max_freq=8000, mfcc_dim=13,\n",
        "        minibatch_size=20, desc_file=None, spectrogram=True, max_duration=10.0, \n",
        "        sort_by_duration=False, n_fft=2048):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "            step (int): Step size in milliseconds between windows (for spectrogram ONLY)\n",
        "            window (int): FFT window size in milliseconds (for spectrogram ONLY)\n",
        "            max_freq (int): Only FFT bins corresponding to frequencies between\n",
        "                [0, max_freq] are returned (for spectrogram ONLY)\n",
        "            desc_file (str, optional): Path to a JSON-line file that contains\n",
        "                labels and paths to the audio files. If this is None, then\n",
        "                load metadata right away\n",
        "        \"\"\"\n",
        "        self.n_fft = n_fft\n",
        "        self.feat_dim = int(n_fft/2) +1\n",
        "        '''\n",
        "        self.feat_dim = calc_feat_dim(window, max_freq)\n",
        "        '''\n",
        "        self.mfcc_dim = mfcc_dim\n",
        "        self.feats_mean = np.zeros((self.feat_dim,))\n",
        "        self.feats_std = np.ones((self.feat_dim,))\n",
        "        self.rng = random.Random(RNG_SEED)\n",
        "        if desc_file is not None:\n",
        "            self.load_metadata_from_desc_file(desc_file)\n",
        "        self.step = step\n",
        "        self.window = window\n",
        "        self.max_freq = max_freq\n",
        "        self.cur_train_index = 0\n",
        "        self.cur_valid_index = 0\n",
        "        self.cur_test_index = 0\n",
        "        self.max_duration=max_duration\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.spectrogram = spectrogram\n",
        "        self.sort_by_duration = sort_by_duration\n",
        "\n",
        "    def get_batch(self, partition):\n",
        "        \"\"\" Obtain a batch of train, validation, or test data\n",
        "        \"\"\"\n",
        "        if partition == 'train':\n",
        "            audio_paths = self.train_audio_paths\n",
        "            cur_index = self.cur_train_index\n",
        "            texts = self.train_texts\n",
        "        elif partition == 'valid':\n",
        "            audio_paths = self.valid_audio_paths\n",
        "            cur_index = self.cur_valid_index\n",
        "            texts = self.valid_texts\n",
        "        elif partition == 'test':\n",
        "            audio_paths = self.test_audio_paths\n",
        "            cur_index = self.test_valid_index\n",
        "            texts = self.test_texts\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition. \"\n",
        "                \"Must be train/validation\")\n",
        "\n",
        "        features = [self.normalize(self.featurize(a)) for a in \n",
        "            audio_paths[cur_index:cur_index+self.minibatch_size]]\n",
        "\n",
        "        # calculate necessary sizes\n",
        "        max_length = max([features[i].shape[0] \n",
        "            for i in range(0, self.minibatch_size)])\n",
        "        max_string_length = max([len(texts[cur_index+i]) \n",
        "            for i in range(0, self.minibatch_size)])\n",
        "        \n",
        "        # initialize the arrays\n",
        "        X_data = np.zeros([self.minibatch_size, max_length, self.feat_dim*self.spectrogram + self.mfcc_dim*(not self.spectrogram)])\n",
        "        labels = np.ones([self.minibatch_size, max_string_length]) * 28\n",
        "        input_length = np.zeros([self.minibatch_size, 1])\n",
        "        label_length = np.zeros([self.minibatch_size, 1])\n",
        "        \n",
        "        for i in range(0, self.minibatch_size):\n",
        "            # calculate X_data & input_length\n",
        "            feat = features[i]\n",
        "            input_length[i] = feat.shape[0]\n",
        "            X_data[i, :feat.shape[0], :] = feat\n",
        "\n",
        "            # calculate labels & label_length\n",
        "            label = np.array(text_to_int_sequence(texts[cur_index+i])) \n",
        "            labels[i, :len(label)] = label\n",
        "            label_length[i] = len(label)\n",
        " \n",
        "        # return the arrays\n",
        "        outputs = {'ctc': np.zeros([self.minibatch_size])}\n",
        "        inputs = {'the_input': X_data, \n",
        "                  'the_labels': labels, \n",
        "                  'input_length': input_length, \n",
        "                  'label_length': label_length \n",
        "                 }\n",
        "        return (inputs, outputs)\n",
        "\n",
        "    def shuffle_data_by_partition(self, partition):\n",
        "        \"\"\" Shuffle the training or validation data\n",
        "        \"\"\"\n",
        "        if partition == 'train':\n",
        "            self.train_audio_paths, self.train_durations, self.train_texts = shuffle_data(\n",
        "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
        "        elif partition == 'valid':\n",
        "            self.valid_audio_paths, self.valid_durations, self.valid_texts = shuffle_data(\n",
        "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition. \"\n",
        "                \"Must be train/validation\")\n",
        "\n",
        "    def sort_data_by_duration(self, partition):\n",
        "        \"\"\" Sort the training or validation sets by (increasing) duration\n",
        "        \"\"\"\n",
        "        if partition == 'train':\n",
        "            self.train_audio_paths, self.train_durations, self.train_texts = sort_data(\n",
        "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
        "        elif partition == 'valid':\n",
        "            self.valid_audio_paths, self.valid_durations, self.valid_texts = sort_data(\n",
        "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition. \"\n",
        "                \"Must be train/validation\")\n",
        "\n",
        "    def next_train(self):\n",
        "        \"\"\" Obtain a batch of training data\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            ret = self.get_batch('train')\n",
        "            self.cur_train_index += self.minibatch_size\n",
        "            if self.cur_train_index >= len(self.train_texts) - self.minibatch_size:\n",
        "                self.cur_train_index = 0\n",
        "                self.shuffle_data_by_partition('train')\n",
        "            yield ret    \n",
        "\n",
        "    def next_valid(self):\n",
        "        \"\"\" Obtain a batch of validation data\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            ret = self.get_batch('valid')\n",
        "            self.cur_valid_index += self.minibatch_size\n",
        "            if self.cur_valid_index >= len(self.valid_texts) - self.minibatch_size:\n",
        "                self.cur_valid_index = 0\n",
        "                self.shuffle_data_by_partition('valid')\n",
        "            yield ret\n",
        "\n",
        "    def next_test(self):\n",
        "        \"\"\" Obtain a batch of test data\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            ret = self.get_batch('test')\n",
        "            self.cur_test_index += self.minibatch_size\n",
        "            if self.cur_test_index >= len(self.test_texts) - self.minibatch_size:\n",
        "                self.cur_test_index = 0\n",
        "            yield ret\n",
        "\n",
        "    def load_train_data(self, desc_file='train_corpus.json'):\n",
        "        self.load_metadata_from_desc_file(desc_file, 'train')\n",
        "        self.fit_train()\n",
        "        if self.sort_by_duration:\n",
        "            self.sort_data_by_duration('train')\n",
        "\n",
        "    def load_validation_data(self, desc_file='valid_corpus.json'):\n",
        "        self.load_metadata_from_desc_file(desc_file, 'validation')\n",
        "        if self.sort_by_duration:\n",
        "            self.sort_data_by_duration('valid')\n",
        "\n",
        "    def load_test_data(self, desc_file='test_corpus.json'):\n",
        "        self.load_metadata_from_desc_file(desc_file, 'test')\n",
        "    \n",
        "    def load_metadata_from_desc_file(self, desc_file, partition):\n",
        "        \"\"\" Read metadata from a JSON-line file\n",
        "            (possibly takes long, depending on the filesize)\n",
        "        Params:\n",
        "            desc_file (str):  Path to a JSON-line file that contains labels and\n",
        "                paths to the audio files\n",
        "            partition (str): One of 'train', 'validation' or 'test'\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        f = open(desc_file)\n",
        "        file_loaded = json.load(f)\n",
        "        \n",
        "        audio_paths, durations, texts = [], [], []\n",
        "        for speech in file_loaded:\n",
        "            if float(speech[\"duration\"]) > self.max_duration:\n",
        "                continue\n",
        "            audio_paths.append(speech[\"key\"])\n",
        "            durations.append(speech[\"duration\"])\n",
        "\n",
        "            audioText = speech[\"text\"].lower()\n",
        "            audioText = re.sub('[^a-zA-Z0-9 \\n]', '', audioText)\n",
        "            audioText = audioText.strip()\n",
        "            '''\n",
        "            audioText = speech[\"text\"].lower().replace('.', '').replace('?', '').replace(',','').replace(\"'\", '').replace('!', '').replace('-', '')\n",
        "            '''\n",
        "            audioText = convert_num_to_words(audioText)\n",
        "            texts.append(audioText)\n",
        "\n",
        "        if partition == 'train':\n",
        "            self.train_audio_paths = audio_paths\n",
        "            self.train_durations = durations\n",
        "            self.train_texts = texts\n",
        "        elif partition == 'validation':\n",
        "            self.valid_audio_paths = audio_paths\n",
        "            self.valid_durations = durations\n",
        "            self.valid_texts = texts\n",
        "        elif partition == 'test':\n",
        "            self.test_audio_paths = audio_paths\n",
        "            self.test_durations = durations\n",
        "            self.test_texts = texts\n",
        "        else:\n",
        "            raise Exception(\"Invalid partition to load metadata. \"\n",
        "             \"Must be train/validation/test\")\n",
        "            \n",
        "    def fit_train(self, k_samples=100):\n",
        "        \"\"\" Estimate the mean and std of the features from the training set\n",
        "        Params:\n",
        "            k_samples (int): Use this number of samples for estimation\n",
        "        \"\"\"\n",
        "        k_samples = min(k_samples, len(self.train_audio_paths))\n",
        "        samples = self.rng.sample(self.train_audio_paths, k_samples)\n",
        "        feats = [self.featurize(s) for s in samples]\n",
        "        feats = np.vstack(feats)\n",
        "        self.feats_mean = np.mean(feats, axis=0)\n",
        "        self.feats_std = np.std(feats, axis=0)\n",
        "        \n",
        "    def featurize(self, audio_clip):\n",
        "        \"\"\" For a given audio clip, calculate the corresponding feature\n",
        "        Params:\n",
        "            audio_clip (str): Path to the audio clip\n",
        "        \"\"\"\n",
        "        if self.spectrogram:\n",
        "            return to_spectrogram(audio_clip, n_fft=self.n_fft)\n",
        "            '''\n",
        "            return spectrogram_from_file(\n",
        "                audio_clip, step=self.step, window=self.window,\n",
        "                max_freq=self.max_freq)\n",
        "            '''\n",
        "\n",
        "        else:\n",
        "            (rate, sig) = wav.read(audio_clip)\n",
        "            return mfcc(sig, rate, numcep=self.mfcc_dim)\n",
        "\n",
        "    def normalize(self, feature, eps=1e-14):\n",
        "        \"\"\" Center a feature using the mean and std\n",
        "        Params:\n",
        "            feature (numpy.ndarray): Feature to normalize\n",
        "        \"\"\"\n",
        "        return (feature - self.feats_mean) / (self.feats_std + eps)\n"
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICWxyiXqtbi0"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "from python_speech_features import mfcc\n",
        "import librosa\n",
        "import scipy.io.wavfile as wav\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO2Kv-Ket3qs"
      },
      "source": [
        "\"\"\"\n",
        "Defines two dictionaries for converting \n",
        "between text and integer sequences.\n",
        "\"\"\"\n",
        "\n",
        "char_map_str = \"\"\"\n",
        "' 0\n",
        "<SPACE> 1\n",
        "a 2\n",
        "b 3\n",
        "c 4\n",
        "d 5\n",
        "e 6\n",
        "f 7\n",
        "g 8\n",
        "h 9\n",
        "i 10\n",
        "j 11\n",
        "k 12\n",
        "l 13\n",
        "m 14\n",
        "n 15\n",
        "o 16\n",
        "p 17\n",
        "q 18\n",
        "r 19\n",
        "s 20\n",
        "t 21\n",
        "u 22\n",
        "v 23\n",
        "w 24\n",
        "x 25\n",
        "y 26\n",
        "z 27\n",
        "\"\"\"\n",
        "# the \"blank\" character is mapped to 28\n",
        "\n",
        "char_map = {}\n",
        "index_map = {}\n",
        "for line in char_map_str.strip().split('\\n'):\n",
        "    ch, index = line.split()\n",
        "    char_map[ch] = int(index)\n",
        "    index_map[int(index)+1] = ch\n",
        "index_map[2] = ' '"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9WTMZnetium"
      },
      "source": [
        "import numpy as np\n",
        "import soundfile\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import num2words #!pip install num2words\n",
        "\n",
        "def shuffle_data(audio_paths, durations, texts):\n",
        "    \"\"\" Shuffle the data (called after making a complete pass through \n",
        "        training or validation data during the training process)\n",
        "    Params:\n",
        "        audio_paths (list): Paths to audio clips\n",
        "        durations (list): Durations of utterances for each audio clip\n",
        "        texts (list): Sentences uttered in each audio clip\n",
        "    \"\"\"\n",
        "    p = np.random.permutation(len(audio_paths))\n",
        "    audio_paths = [audio_paths[i] for i in p] \n",
        "    durations = [durations[i] for i in p] \n",
        "    texts = [texts[i] for i in p]\n",
        "    return audio_paths, durations, texts\n",
        "\n",
        "def sort_data(audio_paths, durations, texts):\n",
        "    \"\"\" Sort the data by duration \n",
        "    Params:\n",
        "        audio_paths (list): Paths to audio clips\n",
        "        durations (list): Durations of utterances for each audio clip\n",
        "        texts (list): Sentences uttered in each audio clip\n",
        "    \"\"\"\n",
        "    p = np.argsort(durations).tolist()\n",
        "    audio_paths = [audio_paths[i] for i in p]\n",
        "    durations = [durations[i] for i in p] \n",
        "    texts = [texts[i] for i in p]\n",
        "    return audio_paths, durations, texts\n",
        "\n",
        "def vis_train_features(index=0):\n",
        "    \"\"\" Visualizing the data point in the training set at the supplied index\n",
        "    \"\"\"\n",
        "    # obtain spectrogram\n",
        "    audio_gen = AudioGenerator(spectrogram=True)\n",
        "    audio_gen.load_train_data()\n",
        "    vis_audio_path = audio_gen.train_audio_paths[index]\n",
        "    vis_spectrogram_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
        "    # obtain mfcc\n",
        "    audio_gen = AudioGenerator(spectrogram=False)\n",
        "    audio_gen.load_train_data()\n",
        "    vis_mfcc_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
        "    # obtain text label\n",
        "    vis_text = audio_gen.train_texts[index]\n",
        "    # obtain raw audio\n",
        "    vis_raw_audio, _ = librosa.load(vis_audio_path)\n",
        "    # print total number of training examples\n",
        "    print('There are %d total training examples.' % len(audio_gen.train_audio_paths))\n",
        "    # return labels for plotting\n",
        "    return vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path\n",
        "\n",
        "\n",
        "def plot_raw_audio(vis_raw_audio):\n",
        "    # plot the raw audio signal\n",
        "    fig = plt.figure(figsize=(12,3))\n",
        "    ax = fig.add_subplot(111)\n",
        "    steps = len(vis_raw_audio)\n",
        "    ax.plot(np.linspace(1, steps, steps), vis_raw_audio)\n",
        "    plt.title('Audio Signal')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.show()\n",
        "\n",
        "def plot_mfcc_feature(vis_mfcc_feature):\n",
        "    # plot the MFCC feature\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    im = ax.imshow(vis_mfcc_feature, cmap=plt.cm.jet, aspect='auto')\n",
        "    plt.title('Normalized MFCC')\n",
        "    plt.ylabel('Time')\n",
        "    plt.xlabel('MFCC Coefficient')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    ax.set_xticks(np.arange(0, 13, 2), minor=False);\n",
        "    plt.show()\n",
        "\n",
        "def plot_spectrogram_feature(vis_spectrogram_feature):\n",
        "    # plot the normalized spectrogram\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    im = ax.imshow(vis_spectrogram_feature, cmap=plt.cm.jet, aspect='auto')\n",
        "    plt.title('Normalized Spectrogram')\n",
        "    plt.ylabel('Time')\n",
        "    plt.xlabel('Frequency')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    plt.show()\n",
        "\n",
        "def convert_num_to_words(utterance):\n",
        "      utterance = ' '.join([num2words.num2words(i) if i.isdigit() else i for i in utterance.split()])\n",
        "      return utterance\n",
        "\n",
        "def calc_feat_dim(window, max_freq):\n",
        "    return int(0.001 * window * max_freq) + 1\n",
        "\n",
        "def conv_output_length(input_length, filter_size, border_mode, stride,\n",
        "                       dilation=1):\n",
        "    \"\"\" Compute the length of the output sequence after 1D convolution along\n",
        "        time. Note that this function is in line with the function used in\n",
        "        Convolution1D class from Keras.\n",
        "    Params:\n",
        "        input_length (int): Length of the input sequence.\n",
        "        filter_size (int): Width of the convolution kernel.\n",
        "        border_mode (str): Only support `same` or `valid`.\n",
        "        stride (int): Stride size used in 1D convolution.\n",
        "        dilation (int)\n",
        "    \"\"\"\n",
        "    if input_length is None:\n",
        "        return None\n",
        "    assert border_mode in {'same', 'valid'}\n",
        "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
        "    if border_mode == 'same':\n",
        "        output_length = input_length\n",
        "    elif border_mode == 'valid':\n",
        "        output_length = input_length - dilated_filter_size + 1\n",
        "    return (output_length + stride - 1) // stride\n",
        "\n",
        "\n",
        "def get_spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n",
        "    \"\"\"\n",
        "    Compute the spectrogram for a real signal.\n",
        "    The parameters follow the naming convention of\n",
        "    matplotlib.mlab.specgram\n",
        "\n",
        "    Args:\n",
        "        samples (1D array): input audio signal\n",
        "        fft_length (int): number of elements in fft window\n",
        "        sample_rate (scalar): sample rate\n",
        "        hop_length (int): hop length (relative offset between neighboring\n",
        "            fft windows).\n",
        "\n",
        "    Returns:\n",
        "        x (2D array): spectrogram [frequency x time]\n",
        "        freq (1D array): frequency of each row in x\n",
        "\n",
        "    Note:\n",
        "        This is a truncating computation e.g. if fft_length=10,\n",
        "        hop_length=5 and the signal has 23 elements, then the\n",
        "        last 3 elements will be truncated.\n",
        "    \"\"\"\n",
        "    assert not np.iscomplexobj(samples), \"Must not pass in complex numbers\"\n",
        "\n",
        "    window = np.hanning(fft_length)[:, None]\n",
        "    window_norm = np.sum(window**2)\n",
        "\n",
        "    # The scaling below follows the convention of\n",
        "    # matplotlib.mlab.specgram which is the same as\n",
        "    # matlabs specgram.\n",
        "    scale = window_norm * sample_rate\n",
        "\n",
        "    trunc = (len(samples) - fft_length) % hop_length\n",
        "    x = samples[:len(samples) - trunc]\n",
        "\n",
        "    # \"stride trick\" reshape to include overlap\n",
        "    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n",
        "    nstrides = (x.strides[0], x.strides[0] * hop_length)\n",
        "    x = as_strided(x, shape=nshape, strides=nstrides)\n",
        "\n",
        "    # window stride sanity check\n",
        "    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n",
        "\n",
        "    # broadcast window, compute fft over columns and square mod\n",
        "    x = np.fft.rfft(x * window, axis=0)\n",
        "    x = np.absolute(x)**2\n",
        "\n",
        "    # scale, 2.0 for everything except dc and fft_length/2\n",
        "    x[1:-1, :] *= (2.0 / scale)\n",
        "    x[(0, -1), :] /= scale\n",
        "\n",
        "    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n",
        "\n",
        "    return x, freqs\n",
        "\n",
        "\n",
        "def spectrogram_from_file(filename, step=10, window=20, max_freq=None,\n",
        "                          eps=1e-14):\n",
        "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
        "    Params:\n",
        "        filename (str): Path to the audio file\n",
        "        step (int): Step size in milliseconds between windows\n",
        "        window (int): FFT window size in milliseconds\n",
        "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
        "            [0, max_freq] are returned\n",
        "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
        "    \"\"\"\n",
        "    with soundfile.SoundFile(filename) as sound_file:\n",
        "        audio = sound_file.read(dtype='float32')\n",
        "        sample_rate = sound_file.samplerate\n",
        "        print(audio.ndim)\n",
        "        if audio.ndim >= 2:\n",
        "            audio = np.mean(audio, 1)\n",
        "        if max_freq is None:\n",
        "            max_freq = sample_rate / 2\n",
        "\n",
        "        if max_freq > sample_rate / 2:\n",
        "            print(\"Error at: \", filename)\n",
        "            raise ValueError(\"max_freq must not be greater than half of \"\n",
        "                             \" sample rate\")\n",
        "        if step > window:\n",
        "            print(\"Error at: \", filename)\n",
        "            raise ValueError(\"step size must not be greater than window size\")\n",
        "\n",
        "        hop_length = int(0.001 * step * sample_rate)\n",
        "        fft_length = int(0.001 * window * sample_rate)\n",
        "        pxx, freqs = get_spectrogram(audio, fft_length=fft_length, sample_rate=sample_rate, hop_length=hop_length)\n",
        "        ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
        "    \n",
        "    spectroGram = np.transpose(np.log(pxx[:ind, :] + eps))\n",
        "\n",
        "    print(\"Filename         : \", filename)\n",
        "    print(\"PXX shape: \", pxx.shape, \", IND: \", ind, \", freq: \", freqs.shape, \", Max_freq: \", max_freq)\n",
        "    print(\"Sample Rate: \", sample_rate, \", Hop Length: \", hop_length, \", FFT Length: \", fft_length)\n",
        "    print(\"Spectrogram shape: \", spectroGram.shape)\n",
        "\n",
        "    return spectroGram\n",
        "\n",
        "def text_to_int_sequence(text):\n",
        "    \"\"\" Convert text to an integer sequence \"\"\"\n",
        "    int_sequence = []\n",
        "    for c in text:\n",
        "        if c == ' ':\n",
        "            ch = char_map['<SPACE>']\n",
        "        else:\n",
        "            ch = char_map[c]\n",
        "        int_sequence.append(ch)\n",
        "    return int_sequence\n",
        "\n",
        "def int_sequence_to_text(int_sequence):\n",
        "    \"\"\" Convert an integer sequence to text \"\"\"\n",
        "    text = []\n",
        "    for c in int_sequence:\n",
        "        ch = index_map[c]\n",
        "        text.append(ch)\n",
        "    return text\n",
        "\n"
      ],
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0XK7hSQ2pLG"
      },
      "source": [
        "#### Function: Model Build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32vjQi3AC8q8"
      },
      "source": [
        "def cnn_output_length(input_length, filter_size, border_mode, stride,\n",
        "                       dilation=1):\n",
        "    \"\"\" Compute the length of the output sequence after 1D convolution along\n",
        "        time. Note that this function is in line with the function used in\n",
        "        Convolution1D class from Keras.\n",
        "    Params:\n",
        "        input_length (int): Length of the input sequence.\n",
        "        filter_size (int): Width of the convolution kernel.\n",
        "        border_mode (str): Only support `same` or `valid`.\n",
        "        stride (int): Stride size used in 1D convolution.\n",
        "        dilation (int)\n",
        "    \"\"\"\n",
        "    if input_length is None:\n",
        "        return None\n",
        "    assert border_mode in {'same', 'valid'}\n",
        "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
        "    if border_mode == 'same':\n",
        "        output_length = input_length\n",
        "    elif border_mode == 'valid':\n",
        "        output_length = input_length - dilated_filter_size + 1\n",
        "    return (output_length + stride - 1) // stride"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXkxLiPAxHSw"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
        "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM, Dropout)\n",
        "\n",
        "def final_model(input_dim, filters, kernel_size, conv_stride,\n",
        "    conv_border_mode, units, output_dim=29):\n",
        "    \"\"\" Build a deep network for speech \n",
        "    \"\"\"\n",
        "    # Main acoustic input\n",
        "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
        "    # TODO: Specify the layers in your network\n",
        "    # Add convolutional layer\n",
        "    conv_1d = Conv1D(filters, kernel_size, \n",
        "                     strides=conv_stride, \n",
        "                     padding=conv_border_mode,\n",
        "                     activation='relu',\n",
        "                     name='conv1d')(input_data)\n",
        "    # Add batch normalization\n",
        "    bn_cnn = BatchNormalization(name='bn_conv_1d')(conv_1d)\n",
        "    # Add a recurrent layer\n",
        "    dropout1 = Dropout(0.3)(bn_cnn)\n",
        "    simp_rnn1 = GRU(units, return_sequences=True)(dropout1)\n",
        "    bn_rnn1 = BatchNormalization(name='bn_conv_1d1')(simp_rnn1)\n",
        "    \n",
        "    dropout2 = Dropout(0.3)(bn_rnn1)\n",
        "    simp_rnn2 = GRU(units, return_sequences=True)(dropout2)\n",
        "    bn_rnn2 = BatchNormalization(name='bn_conv_1d2')(simp_rnn2)\n",
        "\n",
        "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
        "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn2)\n",
        "\n",
        "    # TODO: Add softmax activation layer\n",
        "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
        "    # Specify the model\n",
        "    model = Model(inputs=input_data, outputs=y_pred)\n",
        "    # TODO: Specify model.output_length\n",
        "    model.output_length = lambda x: cnn_output_length(\n",
        "        x, kernel_size, conv_border_mode, conv_stride)\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwHKQHl-3PcB",
        "outputId": "4462f2de-9d2a-40b3-c365-c3c0cc8157ba"
      },
      "source": [
        "model = final_model(input_dim=1025, # change to 13 if you would like to use MFCC features\n",
        "                        filters=200,\n",
        "                        kernel_size=11, \n",
        "                        conv_stride=2,\n",
        "                        conv_border_mode='valid',\n",
        "                        units=200)"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "the_input (InputLayer)       [(None, None, 1025)]      0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 200)         2255200   \n",
            "_________________________________________________________________\n",
            "bn_conv_1d (BatchNormalizati (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_20 (GRU)                 (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d1 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, None, 200)         0         \n",
            "_________________________________________________________________\n",
            "gru_21 (GRU)                 (None, None, 200)         241200    \n",
            "_________________________________________________________________\n",
            "bn_conv_1d2 (BatchNormalizat (None, None, 200)         800       \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, None, 29)          5829      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, None, 29)          0         \n",
            "=================================================================\n",
            "Total params: 2,745,829\n",
            "Trainable params: 2,744,629\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5gIZcX-6Ee5"
      },
      "source": [
        "import _pickle as pickle\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import (Input, Lambda)\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint   \n",
        "import os\n",
        "\n",
        "input_to_softmax = model\n",
        "train_json='train_corpus.json'\n",
        "valid_json='valid_corpus.json'\n",
        "minibatch_size=20\n",
        "spectrogram=True\n",
        "mfcc_dim=13\n",
        "optimizer=SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
        "epochs=20\n",
        "verbose=1\n",
        "sort_by_duration=False\n",
        "max_duration=10.0"
      ],
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okLRHHWl6c4l"
      },
      "source": [
        "audio_gen = AudioGenerator(minibatch_size=minibatch_size, \n",
        "        spectrogram=spectrogram, mfcc_dim=mfcc_dim, max_duration=max_duration,\n",
        "        sort_by_duration=sort_by_duration)"
      ],
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1_HtcHr6onB"
      },
      "source": [
        "audio_gen.load_train_data(train_json) # Load Metat data from json and save in lists, then fit_train()\n",
        "audio_gen.load_validation_data(valid_json)"
      ],
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18K5yqXe6u8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f20f671-be62-4f98-eb04-23ae5367ce2c"
      },
      "source": [
        "num_train_examples=len(audio_gen.train_audio_paths)\n",
        "print(\"Printing number of Training Data\")\n",
        "print(\"Number of train data: \", num_train_examples)\n",
        "steps_per_epoch = num_train_examples//minibatch_size\n",
        "print(\"Steps per epoch: \", steps_per_epoch)"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing number of Training Data\n",
            "Number of train data:  169\n",
            "Steps per epoch:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91US5qtFAr66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aec38df-7bc1-447e-b5be-34aff615dbed"
      },
      "source": [
        " # calculate validation_steps\n",
        "num_valid_samples = len(audio_gen.valid_audio_paths)\n",
        "print(\"Printing number of Validation Data\")\n",
        "print(\"Number of train data: \", num_valid_samples) \n",
        "validation_steps = num_valid_samples//minibatch_size\n",
        "print(\"Steps per epoch: \", validation_steps)"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing number of Validation Data\n",
            "Number of train data:  149\n",
            "Steps per epoch:  7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArMCU_v4BJRO"
      },
      "source": [
        "# add CTC loss to the NN specified in input_to_softmax\n",
        "model = add_ctc_loss(input_to_softmax) #Added Ctc loss\n",
        "\n",
        "# CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)"
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzRJ88E0DEq8"
      },
      "source": [
        "if not os.path.exists('results'):\n",
        "        os.makedirs('results')"
      ],
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKtbpoqtDco6"
      },
      "source": [
        "pickle_path = 'model_end.pickle' \n",
        "save_model_path ='model_end.h5'\n",
        "\n",
        "# add checkpointer\n",
        "checkpointer = ModelCheckpoint(filepath='results/'+save_model_path, verbose=0)"
      ],
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by4NrZsGEGm1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "160c50a6-dbfe-4499-f3f1-f699c4a38845"
      },
      "source": [
        "hist = model.fit_generator(generator=audio_gen.next_train(), steps_per_epoch=steps_per_epoch,\n",
        "        epochs=epochs, validation_data=audio_gen.next_valid(), validation_steps=validation_steps,\n",
        "        callbacks=[checkpointer], verbose=verbose)"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "8/8 [==============================] - ETA: 0s - loss: 232.6245"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-280-a88780d16424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m hist = model.fit_generator(generator=audio_gen.next_train(), steps_per_epoch=steps_per_epoch,\n\u001b[1;32m      2\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         callbacks=[checkpointer], verbose=verbose)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1139\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1142\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  Not enough time for target transition sequence (required: 73, available: 16)14You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\n\t [[node model_21/ctc/CTCLoss (defined at <ipython-input-16-3c40d9e71078>:3) ]]\n  (1) Invalid argument:  Not enough time for target transition sequence (required: 73, available: 16)14You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\n\t [[node model_21/ctc/CTCLoss (defined at <ipython-input-16-3c40d9e71078>:3) ]]\n\t [[model_21/ctc/CTCLoss/_60]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_48322]\n\nFunction call stack:\ntest_function -> test_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LivGg3q-E4o0"
      },
      "source": [
        "train_corpus[]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}